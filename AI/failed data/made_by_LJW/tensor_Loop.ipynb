{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensor Flow를 활용한 데이터 회귀 분석</h3>\n",
    "\n",
    "텐서플로우(TensorFlow)를 사용하여 기후 데이터를 기반으로 뉴스 수를 예측하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import LogCosh\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path : C:\\Users\\limul\\Desktop\\College\\2_grade\\2_semester\\computational_statistics\\teamProject\\2024-2-CSP\\AI\\made_by_LJW\\..\\data_preprocessing\\merged_final_data_7_shifted.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 17055 into shape (1137,2,newaxis)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 51\u001b[0m\n\u001b[0;32m     48\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_scaler\u001b[38;5;241m.\u001b[39mtransform(y_test)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# datetime_x와 datetime_y를 타임스텝으로 처리하기 위해 reshape\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (samples, timesteps=2, features/2)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreshape(X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)     \u001b[38;5;66;03m# (samples, timesteps=2, features/2)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# 학습 과정 정의\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 17055 into shape (1137,2,newaxis)"
     ]
    }
   ],
   "source": [
    "# 출력 파일 설정\n",
    "output_file = f\"model_result.txt\"\n",
    "best_result = [float('inf'), float('inf'), None, None]\n",
    "\n",
    "# 파일 열기\n",
    "f = open(output_file, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for x in range(1,2):\n",
    "    # 데이터 로드\n",
    "    # 뉴스 데이터 로드\n",
    "    datasPath = os.path.join(os.getcwd(), \"..\", \"data_preprocessing\", f\"merged_final_data_{x*7}_shifted.csv\")\n",
    "    print(f\"file path : {datasPath}\")\n",
    "    data = pd.read_csv(datasPath, encoding='utf-8')\n",
    "\n",
    "    # Week_Num 속성 제거\n",
    "    data = data.drop(columns=['Week_Num'])\n",
    "\n",
    "    data.head()\n",
    "\n",
    "    # 입력과 출력 컬럼 정의\n",
    "    X_columns = ['datetime_x', 'datetime_y', 'Article_Num', 'Polution', 'Enviroment_Polution', 'Biodiversity_Loss',\n",
    "                'Acid_Rain', 'Water_Pollution', 'Climate_Crisis', 'Accelerated_Global_Warming',\n",
    "                'Ozone_Layer_Depletion', 'Hazardous_Substance_Leakage', 'Carbon_Dioxide',\n",
    "                'Weekly_News_Count', 'News_Ratio']\n",
    "    y_columns = ['tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'precip', \n",
    "                'windspeed', 'sealevelpressure', 'moonphase']\n",
    "\n",
    "\n",
    "    # String data인 datetime을 Unix Timestamp로 변환\n",
    "    data['datetime_x'] = pd.to_datetime(data['datetime_x']).map(pd.Timestamp.timestamp)\n",
    "    data['datetime_y'] = pd.to_datetime(data['datetime_y']).map(pd.Timestamp.timestamp)\n",
    "\n",
    "    # 데이터 분리\n",
    "    X = data[X_columns]\n",
    "    y = data[y_columns]\n",
    "\n",
    "    # 훈련 및 테스트 데이터 분리\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    # 스케일링\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    X_train = x_scaler.fit_transform(X_train)\n",
    "    y_train = y_scaler.fit_transform(y_train)\n",
    "    \n",
    "    X_test = x_scaler.transform(X_test)\n",
    "    y_test = y_scaler.transform(y_test)\n",
    "\n",
    "    # datetime_x와 datetime_y를 타임스텝으로 처리하기 위해 reshape\n",
    "    X_train = X_train.reshape(X_train.shape[0], 2, -1)  # (samples, timesteps=2, features/2)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 2, -1)     # (samples, timesteps=2, features/2)\n",
    "\n",
    "    # 학습 과정 정의\n",
    "    epoch = 1000\n",
    "    lr = 0.0001\n",
    "\n",
    "    # 사용자 정의 옵티마이저와 손실 함수\n",
    "    # 손실 함수는 이상치에 대해 Log-Cosh Loss함수 사용\n",
    "    # MAE와 유사하지만, 큰 오차에 대해 더 부드럽게 처리 가능\n",
    "    optimizer = Adam(learning_rate=lr)  # Adam 옵티마이저, 학습률 0.001\n",
    "    loss_function = LogCosh()\n",
    "\n",
    "    # # 모델 생성\n",
    "    # model = tf.keras.Sequential([\n",
    "    #     tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    #     Dropout(0.15),\n",
    "    #     tf.keras.layers.Dense(128, activation='relu'),\n",
    "    #     Dropout(0.15),\n",
    "    #     tf.keras.layers.Dense(64, activation='relu'),\n",
    "    #     Dropout(0.15),\n",
    "    #     tf.keras.layers.Dense(32, activation='relu'),\n",
    "    #     Dropout(0.15),\n",
    "    #     tf.keras.layers.Dense(len(y_columns))  # 출력 노드 수는 y_columns의 수와 동일\n",
    "    # ])\n",
    "\n",
    "    # 모델 생성\n",
    "    model = Sequential([\n",
    "        LSTM(256, activation='tanh', return_sequences=True, input_shape=(2, X_train.shape[2])),\n",
    "        Dropout(0.12),\n",
    "        LSTM(128, activation='tanh', return_sequences=False),\n",
    "        Dropout(0.12),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.12),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.12),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.12),\n",
    "        Dense(len(y_columns))  # 출력 노드 수는 y_columns의 수와 동일\n",
    "    ])\n",
    "\n",
    "    # EarlyStopping 콜백 정의\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',  # 검증 손실을 모니터링\n",
    "        patience=10,         # 개선되지 않는 에포크 수 (침체기 기준)\n",
    "        restore_best_weights=True,  # 최적 가중치를 복원\n",
    "        verbose=1            # 종료 시 메시지 출력\n",
    "    )\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['mae'])\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(X_train, y_train, epochs=epoch, batch_size=64, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "    try:\n",
    "        # 모델 평가 \n",
    "        test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "        print(f\"==============={x*7} days matching===============\")\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "        f.write(f\"==============={x*7} days matching===============\\n\")\n",
    "        f.write(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\\n\")\n",
    "\n",
    "        # Best Model 저장\n",
    "        if (best_result[0] > test_loss and best_result[1] > test_mae):\n",
    "            best_result[0] = test_loss \n",
    "            best_result[1] = test_mae\n",
    "            best_result[2] = x*7\n",
    "            best_result[3] = model\n",
    "\n",
    "        # 예측 수행\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # 출력 데이터(y_columns)에 대한 스케일러를 사용해 역변환\n",
    "        y_test_inverse = y_scaler.inverse_transform(y_test)  # 실제 값 복원\n",
    "        y_pred_inverse = y_scaler.inverse_transform(y_pred)  # 예측 값 복원\n",
    "\n",
    "        # 예측 결과 출력\n",
    "        print(\"Sample Predictions:\")\n",
    "        f.write(\"Sample Predictions:\\n\")\n",
    "        for i in range(5):  # 처음 5개의 샘플에 대해 출력\n",
    "            true_values = y_test_inverse[i]  # Numpy 배열에서 직접 값 가져오기\n",
    "            predicted_values = y_pred_inverse[i]  # 예측 값\n",
    "            print(f\"Sample {i+1}:\")\n",
    "            f.write(f\"Sample {i+1}:\\n\")\n",
    "            for attr, true_val, pred_val in zip(y_columns, true_values, predicted_values):\n",
    "                print(f\"  {attr:<18} | True: {true_val} | Predicted: {pred_val}\")\n",
    "                f.write(f\"  {attr:<18} | True: {true_val} | Predicted: {pred_val}\\n\")\n",
    "            print()\n",
    "            f.write(\"\\n\") \n",
    "    finally:\n",
    "        print(\"FI\")\n",
    "\n",
    "print(f\"best model is {best_result[2]}\")\n",
    "print(f\"Test Loss: {best_result[0]:.4f}, Test MAE: {best_result[1]:.4f}\")\n",
    "f.write(f\"best model is {best_result[2]}\\n\")\n",
    "f.write(f\"Test Loss: {best_result[0]:.4f}, Test MAE: {best_result[1]:.4f}\\n\")\n",
    "# 저장할 모델이 best_result[3]에 있음\n",
    "best_model = best_result[3]\n",
    "\n",
    "# 저장 경로 지정\n",
    "model_save_path = \"best_model.h5\"  # 파일 이름 및 경로 설정\n",
    "\n",
    "# 모델 저장\n",
    "best_model.save(model_save_path)\n",
    "\n",
    "print(f\"베스트 모델이 {model_save_path}에 저장되었습니다.\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
