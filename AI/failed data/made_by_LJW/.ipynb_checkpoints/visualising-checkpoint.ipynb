{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c7292d0-f9c7-499d-9e80-9ed1c4394e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import LogCosh\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a06113a-3605-49c6-b75e-afa7580746d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 불러오기\n",
    "model_path = \"best_model.h5\"  # 저장된 모델 경로\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd00983-348d-4e4d-b89a-3ce3ed378631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path : C:\\Users\\limul\\Desktop\\College\\2_grade\\2_semester\\computational_statistics\\teamProject\\2024-2-CSP\\AI\\made_by_LJW\\..\\data_preprocessing\\merged_final_data_168_shifted.csv\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m\n\u001b[0;32m     38\u001b[0m y_scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# X_train = x_scaler.fit_transform(X_train)\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# y_train = y_scaler.fit_transform(y_train)\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[43mx_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m y_test \u001b[38;5;241m=\u001b[39m y_scaler\u001b[38;5;241m.\u001b[39mtransform(y_test)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mX_test\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    322\u001b[0m         )\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:530\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \n\u001b[0;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;124;03m        Transformed data.\u001b[39;00m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    532\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(X)\n\u001b[0;32m    534\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    535\u001b[0m         X,\n\u001b[0;32m    536\u001b[0m         copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    541\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\utils\\validation.py:1661\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not an estimator instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (estimator))\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[1;32m-> 1661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg \u001b[38;5;241m%\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m})\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This MinMaxScaler instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "datasPath = os.path.join(os.getcwd(), \"..\", \"data_preprocessing\", \"merged_final_data_168_shifted.csv\")\n",
    "print(f\"file path : {datasPath}\")\n",
    "data = pd.read_csv(datasPath, encoding='utf-8')\n",
    "\n",
    "# Week_Num 속성 제거\n",
    "data = data.drop(columns=['Week_Num'])\n",
    "\n",
    "# 입력과 출력 컬럼 정의\n",
    "X_columns = ['datetime_x', 'datetime_y', 'Article_Num', 'Polution', 'Enviroment_Polution', 'Biodiversity_Loss',\n",
    "            'Acid_Rain', 'Water_Pollution', 'Climate_Crisis', 'Accelerated_Global_Warming',\n",
    "            'Ozone_Layer_Depletion', 'Hazardous_Substance_Leakage', 'Carbon_Dioxide',\n",
    "            'Weekly_News_Count', 'News_Ratio']\n",
    "y_columns = ['tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'precip', \n",
    "            'windspeed', 'sealevelpressure', 'moonphase']\n",
    "\n",
    "# String data인 datetime을 Unix Timestamp로 변환\n",
    "data['datetime_x'] = pd.to_datetime(data['datetime_x'])  # 날짜를 datetime 객체로 변환\n",
    "data['datetime_x'] = data['datetime_x'].map(pd.Timestamp.timestamp)  # Unix Timestamp로 변환\n",
    "data['datetime_y'] = pd.to_datetime(data['datetime_y'])  # 날짜를 datetime 객체로 변환\n",
    "data['datetime_y'] = data['datetime_y'].map(pd.Timestamp.timestamp)  # Unix Timestamp로 변환\n",
    "\n",
    "# 데이터 분리\n",
    "X = data[X_columns]\n",
    "y = data[y_columns]\n",
    "\n",
    "# 스케일링\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "# 데이터를 학습시켜 MinMaxScaler를 초기화\n",
    "X = x_scaler.fit_transform(X)  # 전체 데이터를 사용해 fit\n",
    "y = y_scaler.fit_transform(y)  # 전체 데이터를 사용해 fit\n",
    "\n",
    "# 데이터를 테스트 데이터로 설정\n",
    "X_test = X\n",
    "y_test = y\n",
    "\n",
    "# 확인 출력\n",
    "print(f\"\\n\\nX_test:\\n{X_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b051ca-6bf2-4025-8887-8b3e2011f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a28fc6-0a13-42ce-b234-1858f837fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일링된 데이터를 원래 값으로 복원 (inverse_transform 사용)\n",
    "y_test_inverse = y_scaler.inverse_transform(y_test)\n",
    "y_pred_inverse = y_scaler.inverse_transform(y_pred)\n",
    "\n",
    "# 모든 출력 변수 비교 (그래프 크기 조정)\n",
    "num_outputs = y_test_inverse.shape[1]  # 출력 변수 수\n",
    "\n",
    "plt.figure(figsize=(12, num_outputs * 3))  # 출력 변수 수에 따라 그래프 높이 조정\n",
    "for i in range(num_outputs):\n",
    "    plt.subplot(num_outputs, 1, i + 1)\n",
    "    plt.plot(y_test_inverse[:, i], label='True Values', marker='o', linestyle='-', markersize=4)\n",
    "    plt.plot(y_pred_inverse[:, i], label='Predicted Values', marker='x', linestyle='--', markersize=4)\n",
    "    plt.title(f'Comparison of True and Predicted Values - Output {i+1}')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c11e7-c30b-43bd-ad61-e0d607d4939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e67d4-2a8f-4986-a574-ca05a39ee318",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
