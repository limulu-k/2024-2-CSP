{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ef4a1-6953-48b6-8e77-5640a1cc2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, MultiHeadAttention, LayerNormalization, Add, Conv1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df31a18-9069-4308-bc61-4057c08c2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pollutants_column : ['TotalPopulation', 'SeoulPopulation', 'BusanPopulation', 'DaeguPopulation', 'InchenPopulation', 'GwanguPopulation', 'DaejeonPopulation', 'UlsanPopulation', 'GyeonggiPopulation', 'GangwonPopulation', 'ChungcheongbukPopulation', 'ChungcheongnamPopulation', 'JeonbukPopulation', 'JeollanamPopulation', 'GyeongsangbukPopulation', 'GyeongsangnamPopulation', 'JejuPopulation', 'year', 'month', 'week']\n",
      "climate_column : ['tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'precip', 'windspeed', 'sealevelpressure', 'moonphase']\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "# 파일 경로\n",
    "\n",
    "pollutants_path = '../../NewData/Weekly_Air_Pollutants.csv'\n",
    "greenhouse_gas_path = '../../NewData/Weekly_Greenhouse_Gas.csv'\n",
    "population_path = '../../NewData/Weekly_Population.csv'\n",
    "power_usage_path = '../../NewData/Weekly_Power_Usage.csv'\n",
    "temperature_path = '../../ClimateDataTeam/climate_data/merged_weekly_avg_temp.csv'\n",
    "target_path = population_path\n",
    "\n",
    "# 데이터 로드\n",
    "pollutants_df = pd.read_csv(target_path)\n",
    "temperature_df = pd.read_csv(temperature_path)\n",
    "\n",
    "# 속성값 정의\n",
    "pollutants_column = pollutants_df.columns.tolist()\n",
    "climate_column = temperature_df.columns.tolist()\n",
    "\n",
    "# datetime 컬럼 변환 및 병합\n",
    "pollutants_df['datetime'] = pd.to_datetime(pollutants_df['datetime'])\n",
    "temperature_df['datetime'] = pd.to_datetime(temperature_df['datetime'])\n",
    "merged_df = pd.merge(pollutants_df, temperature_df, on='datetime', how='inner')\n",
    "\n",
    "# datetime 형변환 및 제거\n",
    "merged_df['year'] = merged_df['datetime'].dt.year\n",
    "merged_df['month'] = merged_df['datetime'].dt.month\n",
    "merged_df['week'] = merged_df['datetime'].dt.isocalendar().week\n",
    "merged_df = merged_df.drop(columns=['datetime'])  # datetime 제거\n",
    "\n",
    "# pollutants_column에 날짜데이터 추가\n",
    "pollutants_column.append('year')\n",
    "pollutants_column.append('month')\n",
    "pollutants_column.append('week')\n",
    "pollutants_column = pollutants_column[1:]\n",
    "climate_column = climate_column[1:]\n",
    "\n",
    "print(f\"pollutants_column : {pollutants_column}\")\n",
    "print(f\"climate_column : {climate_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfa6420-f874-47c7-b7be-53a9d6c6cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X와 y 분리\n",
    "X = merged_df.drop(columns=pollutants_column[:])  # datetime 및 출력 변수 제외\n",
    "y = merged_df[climate_column[:]]  # 출력 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9c86a5-3c20-4b4d-83ff-23f58f880054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ts shape: (1021, 30, 9), y_ts shape: (1021, 9)\n"
     ]
    }
   ],
   "source": [
    "# 2. 시계열 윈도우 생성 함수\n",
    "def create_time_series_features(X, y, lag):\n",
    "    X_features, y_labels = [], []\n",
    "    for i in range(len(X) - lag):\n",
    "        X_features.append(X.iloc[i:i+lag].values)\n",
    "        y_labels.append(y.iloc[i+lag].values)\n",
    "    return np.array(X_features), np.array(y_labels)\n",
    "\n",
    "# 시계열 윈도우 생성\n",
    "lag = 30  # 과거 10주 데이터를 사용\n",
    "X_ts, y_ts = create_time_series_features(X, y, lag)\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"X_ts shape: {X_ts.shape}, y_ts shape: {y_ts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f81db-f142-48ad-a9e6-7cbf39353fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transformer 블록 정의\n",
    "def transformer_block(x, num_heads, key_dim, ff_dim, dropout_rate):\n",
    "    # Multi-Head Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    out1 = Add()([x, attn_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "\n",
    "    # Feed-forward Network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(out1)  # ff_dim이 out1의 feature 크기와 동일하게 설정\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "\n",
    "    # Shape 맞추기 (이 중 하나 선택)\n",
    "    # 1. ff_dim을 out1과 동일하게 맞춤\n",
    "    # ffn_output = Dense(out1.shape[-1], activation=\"relu\")(out1)\n",
    "\n",
    "    # 2. out1의 feature 크기를 ff_dim으로 확장\n",
    "    out1_resized = Dense(ff_dim)(out1)\n",
    "    out2 = Add()([out1_resized, ffn_output])\n",
    "\n",
    "    # Layer Normalization\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3670833-be9b-422d-a2e2-7f405f47866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 모델 생성\n",
    "def create_model(input_shape, output_dim, num_heads=4, key_dim=32, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Transformer Block\n",
    "    x = transformer_block(inputs, num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    # Flatten Transformer Output\n",
    "    x = GlobalAveragePooling1D()(x)  # time_steps 축 제거\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(output_dim)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8201224e-3c05-4349-80e1-690f934c8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ts, y_ts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f078bf-f1bd-4010-a198-960d7cd7e604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 모델 정의 및 학습\n",
    "# EarlyStopping 콜백 정의\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # 기준으로 삼을 값 (예: 검증 손실)\n",
    "    patience=10,          # 몇 epoch 동안 개선이 없으면 중단\n",
    "    restore_best_weights=True  # 가장 성능이 좋았던 가중치를 복원\n",
    ")\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])  # (lag, 입력 특성 수)\n",
    "output_dim = y_ts.shape[1]  # 출력 변수 수\n",
    "model = create_model(input_shape, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f83683f4-158d-4c3f-a1c3-75de2a8288da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "21/21 [==============================] - 2s 14ms/step - loss: 116566.4922 - mae: 144.1053 - val_loss: 116451.4297 - val_mae: 143.5321\n",
      "Epoch 2/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 116338.3281 - mae: 143.8385 - val_loss: 116198.3750 - val_mae: 143.2543\n",
      "Epoch 3/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 116067.7031 - mae: 143.5723 - val_loss: 115940.5625 - val_mae: 142.9753\n",
      "Epoch 4/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 115823.9375 - mae: 143.3071 - val_loss: 115677.2656 - val_mae: 142.6834\n",
      "Epoch 5/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 115571.9375 - mae: 142.9986 - val_loss: 115421.2109 - val_mae: 142.3756\n",
      "Epoch 6/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 115325.8125 - mae: 142.7426 - val_loss: 115165.6797 - val_mae: 142.0689\n",
      "Epoch 7/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 115065.8984 - mae: 142.4465 - val_loss: 114896.1953 - val_mae: 141.7704\n",
      "Epoch 8/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 114812.6641 - mae: 142.1744 - val_loss: 114635.8281 - val_mae: 141.4840\n",
      "Epoch 9/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 114566.9062 - mae: 141.9309 - val_loss: 114366.6562 - val_mae: 141.2177\n",
      "Epoch 10/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 114316.0234 - mae: 141.6904 - val_loss: 114102.7422 - val_mae: 140.9559\n",
      "Epoch 11/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 114094.8438 - mae: 141.4598 - val_loss: 113864.4141 - val_mae: 140.6886\n",
      "Epoch 12/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 113882.1719 - mae: 141.2070 - val_loss: 113643.3672 - val_mae: 140.4242\n",
      "Epoch 13/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 113656.6484 - mae: 140.9345 - val_loss: 113436.7812 - val_mae: 140.1579\n",
      "Epoch 14/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 113470.4531 - mae: 140.7000 - val_loss: 113234.2344 - val_mae: 139.8915\n",
      "Epoch 15/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 113285.8750 - mae: 140.4727 - val_loss: 113041.1797 - val_mae: 139.6295\n",
      "Epoch 16/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 113092.3047 - mae: 140.2003 - val_loss: 112861.4609 - val_mae: 139.3658\n",
      "Epoch 17/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 112941.3984 - mae: 139.9988 - val_loss: 112687.5625 - val_mae: 139.1142\n",
      "Epoch 18/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 112766.3672 - mae: 139.7334 - val_loss: 112519.9844 - val_mae: 138.8643\n",
      "Epoch 19/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 112595.0547 - mae: 139.4967 - val_loss: 112358.8750 - val_mae: 138.6201\n",
      "Epoch 20/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 112435.7188 - mae: 139.2221 - val_loss: 112200.8516 - val_mae: 138.3581\n",
      "Epoch 21/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 112309.5469 - mae: 138.9960 - val_loss: 112046.8047 - val_mae: 138.0860\n",
      "Epoch 22/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 112143.8047 - mae: 138.7344 - val_loss: 111899.8906 - val_mae: 137.8299\n",
      "Epoch 23/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 111996.9609 - mae: 138.4525 - val_loss: 111758.2422 - val_mae: 137.5641\n",
      "Epoch 24/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 111858.2109 - mae: 138.2049 - val_loss: 111620.9141 - val_mae: 137.3040\n",
      "Epoch 25/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 111738.4297 - mae: 137.9654 - val_loss: 111486.2188 - val_mae: 137.0437\n",
      "Epoch 26/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 111609.3516 - mae: 137.7092 - val_loss: 111353.6953 - val_mae: 136.7803\n",
      "Epoch 27/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 111477.2422 - mae: 137.4594 - val_loss: 111223.3438 - val_mae: 136.5166\n",
      "Epoch 28/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 111322.4688 - mae: 137.1705 - val_loss: 111093.7344 - val_mae: 136.2587\n",
      "Epoch 29/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 111205.8438 - mae: 136.9258 - val_loss: 110965.8750 - val_mae: 135.9930\n",
      "Epoch 30/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 111072.2344 - mae: 136.6500 - val_loss: 110837.9609 - val_mae: 135.7326\n",
      "Epoch 31/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 110967.1641 - mae: 136.4365 - val_loss: 110712.2109 - val_mae: 135.4676\n",
      "Epoch 32/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 110811.5859 - mae: 136.1420 - val_loss: 110585.7109 - val_mae: 135.2044\n",
      "Epoch 33/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 110675.4688 - mae: 135.8586 - val_loss: 110460.2188 - val_mae: 134.9397\n",
      "Epoch 34/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 110563.6562 - mae: 135.6221 - val_loss: 110334.6250 - val_mae: 134.6720\n",
      "Epoch 35/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 110448.7734 - mae: 135.3606 - val_loss: 110209.9141 - val_mae: 134.4029\n",
      "Epoch 36/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 110314.5391 - mae: 135.1207 - val_loss: 110084.7422 - val_mae: 134.1367\n",
      "Epoch 37/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 110181.0078 - mae: 134.8286 - val_loss: 109960.1562 - val_mae: 133.8754\n",
      "Epoch 38/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 110057.4375 - mae: 134.5917 - val_loss: 109834.6953 - val_mae: 133.6208\n",
      "Epoch 39/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 109941.3359 - mae: 134.3323 - val_loss: 109710.1953 - val_mae: 133.3557\n",
      "Epoch 40/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 109806.5859 - mae: 134.0657 - val_loss: 109584.6562 - val_mae: 133.0929\n",
      "Epoch 41/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 109642.2422 - mae: 133.7444 - val_loss: 109459.7344 - val_mae: 132.8238\n",
      "Epoch 42/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 109511.3984 - mae: 133.4933 - val_loss: 109332.7891 - val_mae: 132.5679\n",
      "Epoch 43/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 109425.6953 - mae: 133.3060 - val_loss: 109206.5391 - val_mae: 132.3022\n",
      "Epoch 44/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 109286.8594 - mae: 133.0359 - val_loss: 109080.5234 - val_mae: 132.0434\n",
      "Epoch 45/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 109163.3281 - mae: 132.7755 - val_loss: 108954.1250 - val_mae: 131.7782\n",
      "Epoch 46/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 109021.2656 - mae: 132.4772 - val_loss: 108826.6953 - val_mae: 131.5142\n",
      "Epoch 47/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 108907.0391 - mae: 132.2485 - val_loss: 108699.3203 - val_mae: 131.2522\n",
      "Epoch 48/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 108779.8516 - mae: 131.9908 - val_loss: 108571.3203 - val_mae: 130.9947\n",
      "Epoch 49/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 108656.9688 - mae: 131.7501 - val_loss: 108443.0469 - val_mae: 130.7362\n",
      "Epoch 50/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 108506.2969 - mae: 131.4632 - val_loss: 108314.6250 - val_mae: 130.4725\n",
      "Epoch 51/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 108361.3750 - mae: 131.1823 - val_loss: 108184.9766 - val_mae: 130.2143\n",
      "Epoch 52/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 108249.3750 - mae: 130.9329 - val_loss: 108055.7344 - val_mae: 129.9471\n",
      "Epoch 53/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 108075.8516 - mae: 130.6101 - val_loss: 107924.3672 - val_mae: 129.6835\n",
      "Epoch 54/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 107959.2734 - mae: 130.3853 - val_loss: 107794.2578 - val_mae: 129.4203\n",
      "Epoch 55/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 107836.0156 - mae: 130.1368 - val_loss: 107663.0156 - val_mae: 129.1562\n",
      "Epoch 56/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 107706.8984 - mae: 129.8773 - val_loss: 107530.6094 - val_mae: 128.8907\n",
      "Epoch 57/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 107560.2734 - mae: 129.6100 - val_loss: 107398.6797 - val_mae: 128.6349\n",
      "Epoch 58/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 107460.1562 - mae: 129.3948 - val_loss: 107266.6719 - val_mae: 128.3736\n",
      "Epoch 59/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 107321.6953 - mae: 129.1226 - val_loss: 107133.2344 - val_mae: 128.1040\n",
      "Epoch 60/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 107162.3047 - mae: 128.8173 - val_loss: 107000.0703 - val_mae: 127.8362\n",
      "Epoch 61/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 107055.7188 - mae: 128.6001 - val_loss: 106865.8906 - val_mae: 127.5760\n",
      "Epoch 62/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 106883.8438 - mae: 128.2766 - val_loss: 106731.5625 - val_mae: 127.3121\n",
      "Epoch 63/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 106712.7109 - mae: 127.9662 - val_loss: 106596.5156 - val_mae: 127.0441\n",
      "Epoch 64/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 106601.7266 - mae: 127.7538 - val_loss: 106460.0703 - val_mae: 126.7878\n",
      "Epoch 65/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 106525.6719 - mae: 127.5861 - val_loss: 106324.8203 - val_mae: 126.5294\n",
      "Epoch 66/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 106317.9375 - mae: 127.2179 - val_loss: 106188.5703 - val_mae: 126.2644\n",
      "Epoch 67/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 106180.5156 - mae: 126.9670 - val_loss: 106052.0625 - val_mae: 126.0039\n",
      "Epoch 68/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 106082.7578 - mae: 126.7550 - val_loss: 105914.5000 - val_mae: 125.7403\n",
      "Epoch 69/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 105904.0469 - mae: 126.4571 - val_loss: 105776.9609 - val_mae: 125.4821\n",
      "Epoch 70/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 105788.9922 - mae: 126.2438 - val_loss: 105638.1250 - val_mae: 125.2237\n",
      "Epoch 71/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 105644.7109 - mae: 125.9704 - val_loss: 105499.7422 - val_mae: 124.9608\n",
      "Epoch 72/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 105460.9688 - mae: 125.6544 - val_loss: 105359.9375 - val_mae: 124.6984\n",
      "Epoch 73/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 105365.9844 - mae: 125.4723 - val_loss: 105220.3281 - val_mae: 124.4496\n",
      "Epoch 74/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 105192.4453 - mae: 125.1826 - val_loss: 105080.7188 - val_mae: 124.1945\n",
      "Epoch 75/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 105092.7734 - mae: 124.9901 - val_loss: 104940.1328 - val_mae: 123.9412\n",
      "Epoch 76/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 104952.8984 - mae: 124.7298 - val_loss: 104798.8906 - val_mae: 123.6813\n",
      "Epoch 77/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 104749.3281 - mae: 124.3913 - val_loss: 104657.4062 - val_mae: 123.4364\n",
      "Epoch 78/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 104661.7422 - mae: 124.2515 - val_loss: 104515.2422 - val_mae: 123.1816\n",
      "Epoch 79/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 104494.6172 - mae: 123.9430 - val_loss: 104374.2422 - val_mae: 122.9352\n",
      "Epoch 80/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 104313.3516 - mae: 123.6469 - val_loss: 104230.8750 - val_mae: 122.6873\n",
      "Epoch 81/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 104210.9844 - mae: 123.4903 - val_loss: 104087.5703 - val_mae: 122.4450\n",
      "Epoch 82/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 104035.3516 - mae: 123.2053 - val_loss: 103944.6250 - val_mae: 122.2075\n",
      "Epoch 83/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 103942.7969 - mae: 123.0235 - val_loss: 103801.1484 - val_mae: 121.9586\n",
      "Epoch 84/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 103793.1016 - mae: 122.7905 - val_loss: 103657.4141 - val_mae: 121.7181\n",
      "Epoch 85/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 103665.4844 - mae: 122.5504 - val_loss: 103512.6797 - val_mae: 121.4789\n",
      "Epoch 86/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 103498.4297 - mae: 122.3139 - val_loss: 103367.9062 - val_mae: 121.2390\n",
      "Epoch 87/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 103359.7812 - mae: 122.0281 - val_loss: 103222.5469 - val_mae: 121.0072\n",
      "Epoch 88/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 103172.3047 - mae: 121.7712 - val_loss: 103077.0234 - val_mae: 120.7773\n",
      "Epoch 89/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 103030.1094 - mae: 121.5825 - val_loss: 102930.5938 - val_mae: 120.5480\n",
      "Epoch 90/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 102833.0859 - mae: 121.2501 - val_loss: 102784.5469 - val_mae: 120.3131\n",
      "Epoch 91/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 102732.6250 - mae: 121.1076 - val_loss: 102636.6953 - val_mae: 120.0930\n",
      "Epoch 92/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 102583.1250 - mae: 120.8923 - val_loss: 102489.7344 - val_mae: 119.8722\n",
      "Epoch 93/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 102446.5703 - mae: 120.6700 - val_loss: 102342.5625 - val_mae: 119.6450\n",
      "Epoch 94/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 102296.6484 - mae: 120.4129 - val_loss: 102194.8750 - val_mae: 119.4231\n",
      "Epoch 95/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 102188.9688 - mae: 120.2823 - val_loss: 102047.3672 - val_mae: 119.2096\n",
      "Epoch 96/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 102007.1250 - mae: 119.9922 - val_loss: 101898.8203 - val_mae: 119.0003\n",
      "Epoch 97/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 101877.7422 - mae: 119.8323 - val_loss: 101750.2891 - val_mae: 118.7908\n",
      "Epoch 98/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 101675.8984 - mae: 119.5642 - val_loss: 101602.6562 - val_mae: 118.5832\n",
      "Epoch 99/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 101551.4219 - mae: 119.3817 - val_loss: 101452.6562 - val_mae: 118.3725\n",
      "Epoch 100/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 101400.4609 - mae: 119.1526 - val_loss: 101302.3750 - val_mae: 118.1640\n",
      "Epoch 101/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 101248.6172 - mae: 118.9382 - val_loss: 101152.4922 - val_mae: 117.9619\n",
      "Epoch 102/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 101061.2734 - mae: 118.6545 - val_loss: 101002.4453 - val_mae: 117.7593\n",
      "Epoch 103/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 100883.1797 - mae: 118.5109 - val_loss: 100851.2266 - val_mae: 117.5649\n",
      "Epoch 104/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 100760.1953 - mae: 118.2991 - val_loss: 100699.4219 - val_mae: 117.3714\n",
      "Epoch 105/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 100614.8281 - mae: 118.1035 - val_loss: 100547.9609 - val_mae: 117.1773\n",
      "Epoch 106/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 100448.9297 - mae: 117.8885 - val_loss: 100396.3828 - val_mae: 116.9876\n",
      "Epoch 107/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 100329.3516 - mae: 117.7335 - val_loss: 100244.5469 - val_mae: 116.7970\n",
      "Epoch 108/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 100159.4688 - mae: 117.5452 - val_loss: 100092.0703 - val_mae: 116.6055\n",
      "Epoch 109/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 100008.6562 - mae: 117.4031 - val_loss: 99938.9844 - val_mae: 116.4242\n",
      "Epoch 110/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 99887.6250 - mae: 117.2051 - val_loss: 99786.9766 - val_mae: 116.2414\n",
      "Epoch 111/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 99727.7422 - mae: 116.9831 - val_loss: 99634.4609 - val_mae: 116.0638\n",
      "Epoch 112/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 99536.5078 - mae: 116.7109 - val_loss: 99480.4453 - val_mae: 115.8904\n",
      "Epoch 113/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 99414.6719 - mae: 116.5929 - val_loss: 99326.2188 - val_mae: 115.7108\n",
      "Epoch 114/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 99259.8906 - mae: 116.4573 - val_loss: 99172.4375 - val_mae: 115.5405\n",
      "Epoch 115/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 99053.7500 - mae: 116.1945 - val_loss: 99018.6875 - val_mae: 115.3695\n",
      "Epoch 116/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 98981.9844 - mae: 116.1425 - val_loss: 98863.9531 - val_mae: 115.2016\n",
      "Epoch 117/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 98803.0000 - mae: 115.8929 - val_loss: 98710.2422 - val_mae: 115.0348\n",
      "Epoch 118/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 98643.1875 - mae: 115.8197 - val_loss: 98555.0234 - val_mae: 114.8694\n",
      "Epoch 119/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 98537.7969 - mae: 115.6066 - val_loss: 98400.0547 - val_mae: 114.7034\n",
      "Epoch 120/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 98267.2734 - mae: 115.2776 - val_loss: 98243.7344 - val_mae: 114.5379\n",
      "Epoch 121/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 98109.7031 - mae: 115.1201 - val_loss: 98087.6562 - val_mae: 114.3793\n",
      "Epoch 122/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 97995.6484 - mae: 115.0143 - val_loss: 97931.0312 - val_mae: 114.2204\n",
      "Epoch 123/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 97827.7031 - mae: 114.8707 - val_loss: 97775.0391 - val_mae: 114.0654\n",
      "Epoch 124/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 97720.9062 - mae: 114.6953 - val_loss: 97618.4531 - val_mae: 113.9145\n",
      "Epoch 125/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 97502.3203 - mae: 114.4963 - val_loss: 97460.8750 - val_mae: 113.7640\n",
      "Epoch 126/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 97389.9375 - mae: 114.4177 - val_loss: 97304.1172 - val_mae: 113.6210\n",
      "Epoch 127/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 97194.9609 - mae: 114.1799 - val_loss: 97146.5234 - val_mae: 113.4769\n",
      "Epoch 128/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 97012.6562 - mae: 114.0413 - val_loss: 96988.2109 - val_mae: 113.3329\n",
      "Epoch 129/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 96845.2188 - mae: 113.8008 - val_loss: 96830.1875 - val_mae: 113.1896\n",
      "Epoch 130/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 96727.1250 - mae: 113.7269 - val_loss: 96671.5469 - val_mae: 113.0462\n",
      "Epoch 131/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 96592.1250 - mae: 113.5953 - val_loss: 96513.4844 - val_mae: 112.9107\n",
      "Epoch 132/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 96445.5000 - mae: 113.4593 - val_loss: 96354.1250 - val_mae: 112.7734\n",
      "Epoch 133/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 96270.4375 - mae: 113.3104 - val_loss: 96195.1953 - val_mae: 112.6380\n",
      "Epoch 134/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 96105.6484 - mae: 113.1700 - val_loss: 96035.2344 - val_mae: 112.5052\n",
      "Epoch 135/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 95927.9844 - mae: 112.9982 - val_loss: 95875.0703 - val_mae: 112.3728\n",
      "Epoch 136/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 95792.7344 - mae: 112.8926 - val_loss: 95715.0469 - val_mae: 112.2414\n",
      "Epoch 137/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 95566.4141 - mae: 112.6434 - val_loss: 95554.2500 - val_mae: 112.1148\n",
      "Epoch 138/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 95446.7266 - mae: 112.6013 - val_loss: 95393.9609 - val_mae: 111.9905\n",
      "Epoch 139/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 95276.3125 - mae: 112.4114 - val_loss: 95232.5938 - val_mae: 111.8641\n",
      "Epoch 140/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 95119.7266 - mae: 112.3158 - val_loss: 95071.4297 - val_mae: 111.7380\n",
      "Epoch 141/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 94899.6562 - mae: 112.1305 - val_loss: 94909.3594 - val_mae: 111.6199\n",
      "Epoch 142/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 94748.9922 - mae: 112.0339 - val_loss: 94747.1562 - val_mae: 111.5004\n",
      "Epoch 143/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 94608.1562 - mae: 111.9020 - val_loss: 94585.4609 - val_mae: 111.3781\n",
      "Epoch 144/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 94381.7266 - mae: 111.7053 - val_loss: 94422.4062 - val_mae: 111.2646\n",
      "Epoch 145/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 94290.8516 - mae: 111.6859 - val_loss: 94258.2109 - val_mae: 111.1418\n",
      "Epoch 146/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 94119.9297 - mae: 111.5313 - val_loss: 94095.2656 - val_mae: 111.0270\n",
      "Epoch 147/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 94002.2109 - mae: 111.3795 - val_loss: 93933.0391 - val_mae: 110.9070\n",
      "Epoch 148/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 93784.0234 - mae: 111.2155 - val_loss: 93769.1250 - val_mae: 110.7927\n",
      "Epoch 149/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 93625.9844 - mae: 111.1303 - val_loss: 93604.6797 - val_mae: 110.6799\n",
      "Epoch 150/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 93522.1953 - mae: 111.1031 - val_loss: 93440.1406 - val_mae: 110.5723\n",
      "Epoch 151/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 93275.1719 - mae: 110.9263 - val_loss: 93275.8828 - val_mae: 110.4583\n",
      "Epoch 152/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 93150.8047 - mae: 110.8290 - val_loss: 93110.6094 - val_mae: 110.3495\n",
      "Epoch 153/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 93020.6172 - mae: 110.7491 - val_loss: 92946.3438 - val_mae: 110.2431\n",
      "Epoch 154/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 92835.3594 - mae: 110.5011 - val_loss: 92779.5078 - val_mae: 110.1368\n",
      "Epoch 155/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 92678.8359 - mae: 110.4137 - val_loss: 92615.4844 - val_mae: 110.0356\n",
      "Epoch 156/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 92508.1562 - mae: 110.3139 - val_loss: 92448.9531 - val_mae: 109.9293\n",
      "Epoch 157/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 92304.3750 - mae: 110.2578 - val_loss: 92281.5469 - val_mae: 109.8235\n",
      "Epoch 158/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 92235.5859 - mae: 110.2296 - val_loss: 92114.9219 - val_mae: 109.7217\n",
      "Epoch 159/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 91985.1328 - mae: 109.9945 - val_loss: 91948.8672 - val_mae: 109.6211\n",
      "Epoch 160/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 91813.0859 - mae: 109.9418 - val_loss: 91781.1562 - val_mae: 109.5255\n",
      "Epoch 161/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 91680.7656 - mae: 109.8623 - val_loss: 91612.5859 - val_mae: 109.4253\n",
      "Epoch 162/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 91434.0547 - mae: 109.6290 - val_loss: 91444.6641 - val_mae: 109.3216\n",
      "Epoch 163/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 91326.8281 - mae: 109.5725 - val_loss: 91276.8906 - val_mae: 109.2303\n",
      "Epoch 164/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 91096.9141 - mae: 109.4375 - val_loss: 91108.0078 - val_mae: 109.1242\n",
      "Epoch 165/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 91037.3594 - mae: 109.3456 - val_loss: 90938.7344 - val_mae: 109.0297\n",
      "Epoch 166/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 90820.2578 - mae: 109.2214 - val_loss: 90770.1953 - val_mae: 108.9324\n",
      "Epoch 167/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 90639.9375 - mae: 109.1859 - val_loss: 90599.2109 - val_mae: 108.8354\n",
      "Epoch 168/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 90373.6953 - mae: 108.9507 - val_loss: 90429.6797 - val_mae: 108.7413\n",
      "Epoch 169/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 90251.5312 - mae: 108.8941 - val_loss: 90258.9297 - val_mae: 108.6414\n",
      "Epoch 170/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 90062.2031 - mae: 108.7534 - val_loss: 90089.3984 - val_mae: 108.5444\n",
      "Epoch 171/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 89966.4688 - mae: 108.8040 - val_loss: 89916.7344 - val_mae: 108.4476\n",
      "Epoch 172/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 89787.3438 - mae: 108.6498 - val_loss: 89746.9531 - val_mae: 108.3482\n",
      "Epoch 173/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 89633.2422 - mae: 108.5295 - val_loss: 89575.1562 - val_mae: 108.2549\n",
      "Epoch 174/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 89448.2656 - mae: 108.4680 - val_loss: 89402.8438 - val_mae: 108.1560\n",
      "Epoch 175/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 89201.0938 - mae: 108.3277 - val_loss: 89231.9844 - val_mae: 108.0628\n",
      "Epoch 176/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 89087.9688 - mae: 108.1927 - val_loss: 89058.2344 - val_mae: 107.9671\n",
      "Epoch 177/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 88907.2109 - mae: 108.1141 - val_loss: 88884.3750 - val_mae: 107.8777\n",
      "Epoch 178/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 88758.6016 - mae: 108.0495 - val_loss: 88712.2422 - val_mae: 107.7796\n",
      "Epoch 179/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 88553.2344 - mae: 107.9176 - val_loss: 88538.2656 - val_mae: 107.6854\n",
      "Epoch 180/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 88343.3750 - mae: 107.7896 - val_loss: 88364.5391 - val_mae: 107.5900\n",
      "Epoch 181/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 88301.5547 - mae: 107.7613 - val_loss: 88190.6562 - val_mae: 107.4925\n",
      "Epoch 182/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 87984.1406 - mae: 107.6033 - val_loss: 88016.3438 - val_mae: 107.3986\n",
      "Epoch 183/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 87954.3359 - mae: 107.6036 - val_loss: 87843.0391 - val_mae: 107.3021\n",
      "Epoch 184/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 87741.2656 - mae: 107.5126 - val_loss: 87667.4297 - val_mae: 107.2065\n",
      "Epoch 185/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 87599.4141 - mae: 107.3734 - val_loss: 87492.4766 - val_mae: 107.1118\n",
      "Epoch 186/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 87410.7891 - mae: 107.3244 - val_loss: 87317.5312 - val_mae: 107.0191\n",
      "Epoch 187/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 87110.3359 - mae: 107.0019 - val_loss: 87141.9688 - val_mae: 106.9150\n",
      "Epoch 188/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 87021.0625 - mae: 107.0236 - val_loss: 86965.1016 - val_mae: 106.8205\n",
      "Epoch 189/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 86829.8750 - mae: 107.0078 - val_loss: 86790.1797 - val_mae: 106.7268\n",
      "Epoch 190/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 86619.1016 - mae: 106.9668 - val_loss: 86613.0391 - val_mae: 106.6257\n",
      "Epoch 191/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 86431.0391 - mae: 106.7969 - val_loss: 86435.4766 - val_mae: 106.5258\n",
      "Epoch 192/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 86284.5000 - mae: 106.6609 - val_loss: 86257.9609 - val_mae: 106.4270\n",
      "Epoch 193/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 86186.0547 - mae: 106.6431 - val_loss: 86081.2344 - val_mae: 106.3256\n",
      "Epoch 194/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 85926.5000 - mae: 106.4108 - val_loss: 85902.2188 - val_mae: 106.2276\n",
      "Epoch 195/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 85722.2656 - mae: 106.3887 - val_loss: 85724.0156 - val_mae: 106.1287\n",
      "Epoch 196/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 85601.3984 - mae: 106.3347 - val_loss: 85546.2422 - val_mae: 106.0299\n",
      "Epoch 197/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 85378.6719 - mae: 106.1826 - val_loss: 85366.4375 - val_mae: 105.9280\n",
      "Epoch 198/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 85258.3281 - mae: 106.0156 - val_loss: 85188.8203 - val_mae: 105.8292\n",
      "Epoch 199/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 85038.8984 - mae: 105.8805 - val_loss: 85009.6328 - val_mae: 105.7283\n",
      "Epoch 200/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 84897.4297 - mae: 105.8299 - val_loss: 84828.7344 - val_mae: 105.6206\n",
      "Epoch 201/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 84715.7656 - mae: 105.7907 - val_loss: 84649.5547 - val_mae: 105.5220\n",
      "Epoch 202/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 84530.6094 - mae: 105.6448 - val_loss: 84469.8047 - val_mae: 105.4226\n",
      "Epoch 203/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 84264.5703 - mae: 105.4142 - val_loss: 84288.9844 - val_mae: 105.3207\n",
      "Epoch 204/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 84087.8047 - mae: 105.4461 - val_loss: 84109.2891 - val_mae: 105.2183\n",
      "Epoch 205/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 83961.1953 - mae: 105.4074 - val_loss: 83927.0312 - val_mae: 105.1085\n",
      "Epoch 206/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 83764.1172 - mae: 105.1767 - val_loss: 83744.8203 - val_mae: 105.0035\n",
      "Epoch 207/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 83679.8359 - mae: 105.1523 - val_loss: 83564.9062 - val_mae: 104.9037\n",
      "Epoch 208/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 83392.4609 - mae: 104.9706 - val_loss: 83382.9375 - val_mae: 104.7955\n",
      "Epoch 209/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 83240.5234 - mae: 104.8450 - val_loss: 83201.4531 - val_mae: 104.6966\n",
      "Epoch 210/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 83087.3438 - mae: 104.7628 - val_loss: 83019.7734 - val_mae: 104.5879\n",
      "Epoch 211/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 82881.3281 - mae: 104.7351 - val_loss: 82836.3438 - val_mae: 104.4773\n",
      "Epoch 212/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 82710.8672 - mae: 104.5635 - val_loss: 82653.9844 - val_mae: 104.3776\n",
      "Epoch 213/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 82537.3594 - mae: 104.5266 - val_loss: 82469.9297 - val_mae: 104.2651\n",
      "Epoch 214/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 82417.0703 - mae: 104.4242 - val_loss: 82288.5391 - val_mae: 104.1601\n",
      "Epoch 215/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 82167.6328 - mae: 104.2356 - val_loss: 82104.5156 - val_mae: 104.0564\n",
      "Epoch 216/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 81959.2422 - mae: 104.1719 - val_loss: 81922.3438 - val_mae: 103.9475\n",
      "Epoch 217/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 81807.9531 - mae: 104.0429 - val_loss: 81737.5469 - val_mae: 103.8375\n",
      "Epoch 218/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 81608.6719 - mae: 104.0117 - val_loss: 81552.8906 - val_mae: 103.7257\n",
      "Epoch 219/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 81466.8906 - mae: 103.8728 - val_loss: 81371.3906 - val_mae: 103.6229\n",
      "Epoch 220/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 81148.0156 - mae: 103.7036 - val_loss: 81184.5078 - val_mae: 103.5094\n",
      "Epoch 221/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 81097.9219 - mae: 103.7324 - val_loss: 80999.6562 - val_mae: 103.3997\n",
      "Epoch 222/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 80794.3828 - mae: 103.5585 - val_loss: 80814.6953 - val_mae: 103.2924\n",
      "Epoch 223/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 80631.7266 - mae: 103.4246 - val_loss: 80629.5312 - val_mae: 103.1811\n",
      "Epoch 224/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 80494.1875 - mae: 103.2628 - val_loss: 80444.5859 - val_mae: 103.0693\n",
      "Epoch 225/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 80274.5391 - mae: 103.1728 - val_loss: 80257.0469 - val_mae: 102.9602\n",
      "Epoch 226/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 80129.0547 - mae: 103.0454 - val_loss: 80072.2734 - val_mae: 102.8464\n",
      "Epoch 227/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 79865.1016 - mae: 102.9323 - val_loss: 79886.1172 - val_mae: 102.7337\n",
      "Epoch 228/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 79842.7891 - mae: 103.0108 - val_loss: 79700.5859 - val_mae: 102.6241\n",
      "Epoch 229/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 79587.5547 - mae: 102.8310 - val_loss: 79513.4062 - val_mae: 102.5114\n",
      "Epoch 230/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 79365.0312 - mae: 102.5967 - val_loss: 79328.0078 - val_mae: 102.4037\n",
      "Epoch 231/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 79187.0391 - mae: 102.5523 - val_loss: 79140.5781 - val_mae: 102.2881\n",
      "Epoch 232/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 78935.9844 - mae: 102.3364 - val_loss: 78953.8672 - val_mae: 102.1782\n",
      "Epoch 233/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 78821.8516 - mae: 102.3060 - val_loss: 78766.4453 - val_mae: 102.0659\n",
      "Epoch 234/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 78617.3516 - mae: 102.1950 - val_loss: 78578.3438 - val_mae: 101.9509\n",
      "Epoch 235/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 78414.1641 - mae: 102.0304 - val_loss: 78391.3516 - val_mae: 101.8403\n",
      "Epoch 236/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 78315.0234 - mae: 101.9785 - val_loss: 78203.0703 - val_mae: 101.7273\n",
      "Epoch 237/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 78149.7734 - mae: 101.9992 - val_loss: 78016.1406 - val_mae: 101.6152\n",
      "Epoch 238/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 77899.4766 - mae: 101.7567 - val_loss: 77827.4766 - val_mae: 101.5034\n",
      "Epoch 239/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 77675.3984 - mae: 101.6912 - val_loss: 77638.6328 - val_mae: 101.3871\n",
      "Epoch 240/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 77546.7266 - mae: 101.5098 - val_loss: 77451.0547 - val_mae: 101.2724\n",
      "Epoch 241/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 77429.7109 - mae: 101.4727 - val_loss: 77262.7891 - val_mae: 101.1613\n",
      "Epoch 242/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 77214.6641 - mae: 101.3262 - val_loss: 77074.1562 - val_mae: 101.0421\n",
      "Epoch 243/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 76914.7891 - mae: 101.1692 - val_loss: 76885.6953 - val_mae: 100.9280\n",
      "Epoch 244/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 76775.0156 - mae: 101.0724 - val_loss: 76696.1016 - val_mae: 100.8132\n",
      "Epoch 245/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 76570.7188 - mae: 100.9078 - val_loss: 76506.3516 - val_mae: 100.6994\n",
      "Epoch 246/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 76459.1484 - mae: 100.8917 - val_loss: 76317.3906 - val_mae: 100.5833\n",
      "Epoch 247/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 76189.0625 - mae: 100.7116 - val_loss: 76128.3203 - val_mae: 100.4716\n",
      "Epoch 248/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 76027.1641 - mae: 100.5893 - val_loss: 75938.9609 - val_mae: 100.3534\n",
      "Epoch 249/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 75750.0312 - mae: 100.4709 - val_loss: 75748.2109 - val_mae: 100.2387\n",
      "Epoch 250/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 75656.7812 - mae: 100.4137 - val_loss: 75557.6562 - val_mae: 100.1196\n",
      "Epoch 251/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 75435.8984 - mae: 100.3042 - val_loss: 75367.9062 - val_mae: 100.0039\n",
      "Epoch 252/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 75178.1797 - mae: 100.1688 - val_loss: 75178.1406 - val_mae: 99.8851\n",
      "Epoch 253/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 75105.2344 - mae: 100.0250 - val_loss: 74986.9062 - val_mae: 99.7707\n",
      "Epoch 254/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 74895.1016 - mae: 99.8789 - val_loss: 74795.9531 - val_mae: 99.6510\n",
      "Epoch 255/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 74683.4609 - mae: 99.8074 - val_loss: 74604.9375 - val_mae: 99.5383\n",
      "Epoch 256/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 74600.1016 - mae: 99.6947 - val_loss: 74416.1641 - val_mae: 99.4180\n",
      "Epoch 257/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 74187.6562 - mae: 99.5354 - val_loss: 74223.3594 - val_mae: 99.2997\n",
      "Epoch 258/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 74048.1016 - mae: 99.3773 - val_loss: 74031.7188 - val_mae: 99.1770\n",
      "Epoch 259/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 73869.6172 - mae: 99.3841 - val_loss: 73841.1328 - val_mae: 99.0590\n",
      "Epoch 260/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 73744.1641 - mae: 99.2099 - val_loss: 73649.7891 - val_mae: 98.9433\n",
      "Epoch 261/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 73412.1797 - mae: 98.9936 - val_loss: 73457.4688 - val_mae: 98.8258\n",
      "Epoch 262/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 73341.6484 - mae: 98.9360 - val_loss: 73264.2422 - val_mae: 98.7018\n",
      "Epoch 263/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 73118.8828 - mae: 98.7535 - val_loss: 73073.4531 - val_mae: 98.5843\n",
      "Epoch 264/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 72984.0000 - mae: 98.7592 - val_loss: 72879.9688 - val_mae: 98.4639\n",
      "Epoch 265/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 72867.2734 - mae: 98.7278 - val_loss: 72689.4141 - val_mae: 98.3425\n",
      "Epoch 266/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 72607.0391 - mae: 98.4908 - val_loss: 72497.7734 - val_mae: 98.2255\n",
      "Epoch 267/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 72415.5547 - mae: 98.4495 - val_loss: 72306.3359 - val_mae: 98.1057\n",
      "Epoch 268/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 72268.9922 - mae: 98.3188 - val_loss: 72112.5312 - val_mae: 97.9847\n",
      "Epoch 269/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 71964.7031 - mae: 98.1271 - val_loss: 71919.7578 - val_mae: 97.8610\n",
      "Epoch 270/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 71762.0234 - mae: 98.0641 - val_loss: 71727.2344 - val_mae: 97.7395\n",
      "Epoch 271/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 71691.7734 - mae: 97.9921 - val_loss: 71535.0938 - val_mae: 97.6230\n",
      "Epoch 272/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 71452.5547 - mae: 97.8580 - val_loss: 71341.2656 - val_mae: 97.4986\n",
      "Epoch 273/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 71220.2812 - mae: 97.6133 - val_loss: 71148.3281 - val_mae: 97.3799\n",
      "Epoch 274/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 71069.3828 - mae: 97.6002 - val_loss: 70955.3438 - val_mae: 97.2601\n",
      "Epoch 275/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 70851.5156 - mae: 97.4368 - val_loss: 70761.7656 - val_mae: 97.1340\n",
      "Epoch 276/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 70609.5312 - mae: 97.2479 - val_loss: 70568.4453 - val_mae: 97.0127\n",
      "Epoch 277/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 70452.3828 - mae: 97.1719 - val_loss: 70375.3750 - val_mae: 96.8914\n",
      "Epoch 278/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 70091.4922 - mae: 97.0078 - val_loss: 70180.5547 - val_mae: 96.7639\n",
      "Epoch 279/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 70015.5859 - mae: 96.8733 - val_loss: 69986.8438 - val_mae: 96.6402\n",
      "Epoch 280/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 69835.6562 - mae: 96.7801 - val_loss: 69793.8828 - val_mae: 96.5214\n",
      "Epoch 281/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 69767.0469 - mae: 96.7532 - val_loss: 69599.1562 - val_mae: 96.4011\n",
      "Epoch 282/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 69525.5938 - mae: 96.5903 - val_loss: 69406.0078 - val_mae: 96.2823\n",
      "Epoch 283/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 69323.7891 - mae: 96.5343 - val_loss: 69212.0312 - val_mae: 96.1572\n",
      "Epoch 284/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 69078.8984 - mae: 96.2699 - val_loss: 69018.6172 - val_mae: 96.0334\n",
      "Epoch 285/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 68996.7891 - mae: 96.3238 - val_loss: 68824.2891 - val_mae: 95.9046\n",
      "Epoch 286/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 68804.9375 - mae: 96.1297 - val_loss: 68632.1328 - val_mae: 95.7861\n",
      "Epoch 287/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 68464.1641 - mae: 95.9183 - val_loss: 68435.4375 - val_mae: 95.6582\n",
      "Epoch 288/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 68277.5703 - mae: 95.7845 - val_loss: 68242.2188 - val_mae: 95.5351\n",
      "Epoch 289/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 67953.8594 - mae: 95.5288 - val_loss: 68046.7109 - val_mae: 95.4071\n",
      "Epoch 290/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 68022.4922 - mae: 95.6204 - val_loss: 67852.1328 - val_mae: 95.2818\n",
      "Epoch 291/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 67776.1562 - mae: 95.4821 - val_loss: 67657.4844 - val_mae: 95.1550\n",
      "Epoch 292/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 67537.2969 - mae: 95.3825 - val_loss: 67463.8672 - val_mae: 95.0321\n",
      "Epoch 293/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 67486.1016 - mae: 95.2952 - val_loss: 67268.7344 - val_mae: 94.9113\n",
      "Epoch 294/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 67114.3281 - mae: 94.9980 - val_loss: 67073.2656 - val_mae: 94.7784\n",
      "Epoch 295/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 67011.2031 - mae: 94.9472 - val_loss: 66878.3828 - val_mae: 94.6567\n",
      "Epoch 296/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 66778.2578 - mae: 94.8028 - val_loss: 66684.3281 - val_mae: 94.5283\n",
      "Epoch 297/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 66666.5234 - mae: 94.7489 - val_loss: 66488.2344 - val_mae: 94.4063\n",
      "Epoch 298/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 66315.1250 - mae: 94.5315 - val_loss: 66295.1953 - val_mae: 94.2815\n",
      "Epoch 299/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 66106.9453 - mae: 94.3784 - val_loss: 66099.7656 - val_mae: 94.1492\n",
      "Epoch 300/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 66020.3828 - mae: 94.2608 - val_loss: 65904.4844 - val_mae: 94.0219\n",
      "Epoch 301/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 65751.2734 - mae: 94.2094 - val_loss: 65709.9141 - val_mae: 93.8945\n",
      "Epoch 302/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 65536.9688 - mae: 94.0449 - val_loss: 65513.5781 - val_mae: 93.7682\n",
      "Epoch 303/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 65383.0547 - mae: 93.9244 - val_loss: 65319.1758 - val_mae: 93.6412\n",
      "Epoch 304/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 65187.9766 - mae: 93.7404 - val_loss: 65124.1523 - val_mae: 93.5141\n",
      "Epoch 305/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 64962.5039 - mae: 93.6746 - val_loss: 64927.3672 - val_mae: 93.3859\n",
      "Epoch 306/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 64878.1719 - mae: 93.6252 - val_loss: 64733.0352 - val_mae: 93.2545\n",
      "Epoch 307/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 64597.1797 - mae: 93.3986 - val_loss: 64535.7969 - val_mae: 93.1263\n",
      "Epoch 308/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 64478.4922 - mae: 93.3466 - val_loss: 64343.0195 - val_mae: 93.0029\n",
      "Epoch 309/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 64275.8086 - mae: 93.1461 - val_loss: 64145.6172 - val_mae: 92.8721\n",
      "Epoch 310/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 63994.5352 - mae: 93.0463 - val_loss: 63952.4141 - val_mae: 92.7482\n",
      "Epoch 311/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 63768.2031 - mae: 92.8442 - val_loss: 63756.6328 - val_mae: 92.6150\n",
      "Epoch 312/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 63660.3750 - mae: 92.8162 - val_loss: 63560.3906 - val_mae: 92.4882\n",
      "Epoch 313/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 63327.1836 - mae: 92.4788 - val_loss: 63366.2305 - val_mae: 92.3575\n",
      "Epoch 314/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 63228.2266 - mae: 92.4624 - val_loss: 63169.2812 - val_mae: 92.2228\n",
      "Epoch 315/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 63020.8164 - mae: 92.3629 - val_loss: 62973.3125 - val_mae: 92.0943\n",
      "Epoch 316/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 62912.4492 - mae: 92.2679 - val_loss: 62776.7188 - val_mae: 91.9570\n",
      "Epoch 317/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 62624.7109 - mae: 92.0960 - val_loss: 62581.4336 - val_mae: 91.8302\n",
      "Epoch 318/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 62405.2812 - mae: 91.9622 - val_loss: 62386.4805 - val_mae: 91.7024\n",
      "Epoch 319/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 62243.7852 - mae: 91.8100 - val_loss: 62189.4258 - val_mae: 91.5653\n",
      "Epoch 320/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 62104.7617 - mae: 91.7826 - val_loss: 61994.8672 - val_mae: 91.4400\n",
      "Epoch 321/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 61847.7305 - mae: 91.5409 - val_loss: 61798.9883 - val_mae: 91.3027\n",
      "Epoch 322/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 61737.3438 - mae: 91.3513 - val_loss: 61603.5430 - val_mae: 91.1733\n",
      "Epoch 323/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 61453.1523 - mae: 91.3063 - val_loss: 61407.2070 - val_mae: 91.0381\n",
      "Epoch 324/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 61257.6328 - mae: 91.1889 - val_loss: 61210.0664 - val_mae: 90.9059\n",
      "Epoch 325/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 61120.7227 - mae: 91.1254 - val_loss: 61015.2734 - val_mae: 90.7722\n",
      "Epoch 326/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 60906.0742 - mae: 90.9442 - val_loss: 60820.2578 - val_mae: 90.6479\n",
      "Epoch 327/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 60665.2500 - mae: 90.7972 - val_loss: 60624.7695 - val_mae: 90.5088\n",
      "Epoch 328/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 60720.1094 - mae: 90.8734 - val_loss: 60429.9531 - val_mae: 90.3781\n",
      "Epoch 329/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 60408.0859 - mae: 90.6621 - val_loss: 60233.9805 - val_mae: 90.2476\n",
      "Epoch 330/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 60183.7773 - mae: 90.3412 - val_loss: 60038.5664 - val_mae: 90.1152\n",
      "Epoch 331/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 59862.7500 - mae: 90.2291 - val_loss: 59843.2148 - val_mae: 89.9782\n",
      "Epoch 332/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 59724.3203 - mae: 90.1335 - val_loss: 59646.8359 - val_mae: 89.8479\n",
      "Epoch 333/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 59362.1914 - mae: 89.8954 - val_loss: 59452.3945 - val_mae: 89.7111\n",
      "Epoch 334/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 59331.1641 - mae: 89.8710 - val_loss: 59256.2188 - val_mae: 89.5771\n",
      "Epoch 335/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 59085.5586 - mae: 89.7718 - val_loss: 59059.2812 - val_mae: 89.4400\n",
      "Epoch 336/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 58983.9336 - mae: 89.6386 - val_loss: 58865.5469 - val_mae: 89.3080\n",
      "Epoch 337/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 58702.3867 - mae: 89.4274 - val_loss: 58668.7031 - val_mae: 89.1711\n",
      "Epoch 338/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 58547.8945 - mae: 89.3239 - val_loss: 58473.5430 - val_mae: 89.0404\n",
      "Epoch 339/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 58398.4609 - mae: 89.2241 - val_loss: 58278.2812 - val_mae: 88.9007\n",
      "Epoch 340/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 58245.4922 - mae: 89.0916 - val_loss: 58082.8672 - val_mae: 88.7647\n",
      "Epoch 341/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 58049.8164 - mae: 89.0230 - val_loss: 57886.5742 - val_mae: 88.6332\n",
      "Epoch 342/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 57726.1836 - mae: 88.7214 - val_loss: 57691.3945 - val_mae: 88.4999\n",
      "Epoch 343/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 57444.6211 - mae: 88.4488 - val_loss: 57496.9883 - val_mae: 88.3609\n",
      "Epoch 344/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 57252.0938 - mae: 88.4117 - val_loss: 57300.9805 - val_mae: 88.2245\n",
      "Epoch 345/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 57143.2344 - mae: 88.4088 - val_loss: 57105.1211 - val_mae: 88.0825\n",
      "Epoch 346/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 57010.2070 - mae: 88.3011 - val_loss: 56909.4648 - val_mae: 87.9468\n",
      "Epoch 347/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 56839.7422 - mae: 88.1033 - val_loss: 56716.6094 - val_mae: 87.8175\n",
      "Epoch 348/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 56725.4961 - mae: 88.1133 - val_loss: 56520.8164 - val_mae: 87.6729\n",
      "Epoch 349/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 56405.8359 - mae: 87.9097 - val_loss: 56325.4531 - val_mae: 87.5401\n",
      "Epoch 350/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 56298.7969 - mae: 87.7394 - val_loss: 56131.4883 - val_mae: 87.4017\n",
      "Epoch 351/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 56033.0508 - mae: 87.5579 - val_loss: 55934.6172 - val_mae: 87.2629\n",
      "Epoch 352/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 55819.9766 - mae: 87.3448 - val_loss: 55740.6953 - val_mae: 87.1292\n",
      "Epoch 353/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 55753.9375 - mae: 87.3763 - val_loss: 55545.6758 - val_mae: 86.9903\n",
      "Epoch 354/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 55476.4844 - mae: 87.1217 - val_loss: 55350.0625 - val_mae: 86.8493\n",
      "Epoch 355/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 55098.9805 - mae: 86.9790 - val_loss: 55158.8789 - val_mae: 86.7137\n",
      "Epoch 356/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 54937.7969 - mae: 86.7976 - val_loss: 54961.0078 - val_mae: 86.5729\n",
      "Epoch 357/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 54860.3320 - mae: 86.6829 - val_loss: 54767.6641 - val_mae: 86.4345\n",
      "Epoch 358/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 54581.3203 - mae: 86.5778 - val_loss: 54571.0469 - val_mae: 86.2958\n",
      "Epoch 359/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 54298.4062 - mae: 86.3958 - val_loss: 54377.2148 - val_mae: 86.1541\n",
      "Epoch 360/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 54215.4844 - mae: 86.2773 - val_loss: 54184.2305 - val_mae: 86.0166\n",
      "Epoch 361/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 54180.3789 - mae: 86.2080 - val_loss: 53988.7383 - val_mae: 85.8773\n",
      "Epoch 362/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 53974.2266 - mae: 86.1119 - val_loss: 53794.9688 - val_mae: 85.7385\n",
      "Epoch 363/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 53772.6250 - mae: 85.9787 - val_loss: 53600.0117 - val_mae: 85.5990\n",
      "Epoch 364/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 53593.6875 - mae: 85.7606 - val_loss: 53406.1602 - val_mae: 85.4599\n",
      "Epoch 365/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 53359.5820 - mae: 85.5993 - val_loss: 53213.2852 - val_mae: 85.3209\n",
      "Epoch 366/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 53044.6562 - mae: 85.4655 - val_loss: 53019.5898 - val_mae: 85.1818\n",
      "Epoch 367/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 52922.7227 - mae: 85.3097 - val_loss: 52825.0469 - val_mae: 85.0402\n",
      "Epoch 368/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 52670.9570 - mae: 85.1876 - val_loss: 52631.4766 - val_mae: 84.8967\n",
      "Epoch 369/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 52405.4805 - mae: 84.9196 - val_loss: 52436.2812 - val_mae: 84.7539\n",
      "Epoch 370/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 52254.1602 - mae: 84.8150 - val_loss: 52244.3672 - val_mae: 84.6117\n",
      "Epoch 371/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 52147.8789 - mae: 84.7695 - val_loss: 52050.4766 - val_mae: 84.4708\n",
      "Epoch 372/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 51972.5820 - mae: 84.6611 - val_loss: 51857.4102 - val_mae: 84.3300\n",
      "Epoch 373/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 51860.2227 - mae: 84.5509 - val_loss: 51663.9023 - val_mae: 84.1914\n",
      "Epoch 374/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 51688.2383 - mae: 84.4270 - val_loss: 51471.4766 - val_mae: 84.0514\n",
      "Epoch 375/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 51519.8086 - mae: 84.3502 - val_loss: 51278.8281 - val_mae: 83.9055\n",
      "Epoch 376/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 51066.6641 - mae: 84.0563 - val_loss: 51084.9414 - val_mae: 83.7641\n",
      "Epoch 377/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 50904.6055 - mae: 83.8984 - val_loss: 50891.6875 - val_mae: 83.6210\n",
      "Epoch 378/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 50844.1328 - mae: 83.7846 - val_loss: 50699.1172 - val_mae: 83.4751\n",
      "Epoch 379/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 50529.4688 - mae: 83.6093 - val_loss: 50505.8594 - val_mae: 83.3312\n",
      "Epoch 380/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 50358.2578 - mae: 83.4454 - val_loss: 50312.3359 - val_mae: 83.1875\n",
      "Epoch 381/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 50271.8984 - mae: 83.4252 - val_loss: 50120.3281 - val_mae: 83.0491\n",
      "Epoch 382/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 49901.2656 - mae: 83.1683 - val_loss: 49929.4648 - val_mae: 82.9027\n",
      "Epoch 383/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 49823.9414 - mae: 83.1174 - val_loss: 49737.8398 - val_mae: 82.7578\n",
      "Epoch 384/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 49504.8789 - mae: 82.8250 - val_loss: 49544.3516 - val_mae: 82.6102\n",
      "Epoch 385/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 49433.3203 - mae: 82.6913 - val_loss: 49352.2188 - val_mae: 82.4659\n",
      "Epoch 386/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 49401.4883 - mae: 82.7836 - val_loss: 49160.9531 - val_mae: 82.3255\n",
      "Epoch 387/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 48908.1602 - mae: 82.3048 - val_loss: 48968.6094 - val_mae: 82.1814\n",
      "Epoch 388/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 48810.9727 - mae: 82.2182 - val_loss: 48776.1875 - val_mae: 82.0365\n",
      "Epoch 389/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 48693.8867 - mae: 82.2205 - val_loss: 48584.9961 - val_mae: 81.8923\n",
      "Epoch 390/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 48459.8984 - mae: 82.0166 - val_loss: 48394.1172 - val_mae: 81.7456\n",
      "Epoch 391/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 48307.1602 - mae: 81.8676 - val_loss: 48202.1680 - val_mae: 81.6004\n",
      "Epoch 392/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 48079.0547 - mae: 81.7257 - val_loss: 48013.6914 - val_mae: 81.4576\n",
      "Epoch 393/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 47883.0938 - mae: 81.6041 - val_loss: 47821.9531 - val_mae: 81.3116\n",
      "Epoch 394/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 47729.3906 - mae: 81.3890 - val_loss: 47631.3750 - val_mae: 81.1653\n",
      "Epoch 395/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 47579.0781 - mae: 81.3047 - val_loss: 47439.4023 - val_mae: 81.0192\n",
      "Epoch 396/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 47229.7578 - mae: 80.9983 - val_loss: 47247.5781 - val_mae: 80.8692\n",
      "Epoch 397/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 47286.1602 - mae: 81.1412 - val_loss: 47058.3398 - val_mae: 80.7226\n",
      "Epoch 398/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 47000.3516 - mae: 80.8885 - val_loss: 46868.1367 - val_mae: 80.5739\n",
      "Epoch 399/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 46882.6641 - mae: 80.8061 - val_loss: 46679.8672 - val_mae: 80.4342\n",
      "Epoch 400/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 46594.1094 - mae: 80.5571 - val_loss: 46490.1367 - val_mae: 80.2853\n",
      "Epoch 401/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 46353.5352 - mae: 80.3601 - val_loss: 46298.7852 - val_mae: 80.1338\n",
      "Epoch 402/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 46145.0820 - mae: 80.3028 - val_loss: 46107.1367 - val_mae: 79.9822\n",
      "Epoch 403/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 46025.2930 - mae: 80.1698 - val_loss: 45919.2812 - val_mae: 79.8365\n",
      "Epoch 404/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 45796.1250 - mae: 80.0141 - val_loss: 45729.6289 - val_mae: 79.6931\n",
      "Epoch 405/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 45531.3047 - mae: 79.7900 - val_loss: 45539.3555 - val_mae: 79.5425\n",
      "Epoch 406/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 45514.4648 - mae: 79.7241 - val_loss: 45351.4688 - val_mae: 79.3948\n",
      "Epoch 407/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 45286.5039 - mae: 79.6942 - val_loss: 45163.7227 - val_mae: 79.2515\n",
      "Epoch 408/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 44959.0195 - mae: 79.3300 - val_loss: 44974.6758 - val_mae: 79.0997\n",
      "Epoch 409/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 44907.2539 - mae: 79.3402 - val_loss: 44784.9727 - val_mae: 78.9511\n",
      "Epoch 410/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 44663.8906 - mae: 78.9857 - val_loss: 44597.4844 - val_mae: 78.8048\n",
      "Epoch 411/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 44444.7344 - mae: 78.9345 - val_loss: 44410.4453 - val_mae: 78.6564\n",
      "Epoch 412/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 44235.6016 - mae: 78.6850 - val_loss: 44221.6328 - val_mae: 78.5031\n",
      "Epoch 413/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 44054.5625 - mae: 78.5217 - val_loss: 44034.8320 - val_mae: 78.3575\n",
      "Epoch 414/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 43856.6484 - mae: 78.4403 - val_loss: 43845.9219 - val_mae: 78.2029\n",
      "Epoch 415/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 43933.1758 - mae: 78.4597 - val_loss: 43658.3828 - val_mae: 78.0549\n",
      "Epoch 416/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 43541.1562 - mae: 78.1707 - val_loss: 43470.4805 - val_mae: 77.9078\n",
      "Epoch 417/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 43407.0938 - mae: 78.0671 - val_loss: 43284.1875 - val_mae: 77.7573\n",
      "Epoch 418/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 43185.3945 - mae: 77.8796 - val_loss: 43097.0898 - val_mae: 77.6082\n",
      "Epoch 419/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 43052.3945 - mae: 77.8901 - val_loss: 42908.8398 - val_mae: 77.4552\n",
      "Epoch 420/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 43017.0469 - mae: 77.7450 - val_loss: 42724.3555 - val_mae: 77.3102\n",
      "Epoch 421/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 42746.8906 - mae: 77.5585 - val_loss: 42538.5547 - val_mae: 77.1573\n",
      "Epoch 422/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 42305.3828 - mae: 77.1926 - val_loss: 42353.6719 - val_mae: 77.0056\n",
      "Epoch 423/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 42347.7734 - mae: 77.2503 - val_loss: 42168.3633 - val_mae: 76.8572\n",
      "Epoch 424/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 42072.8477 - mae: 77.0636 - val_loss: 41981.8047 - val_mae: 76.7083\n",
      "Epoch 425/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 42026.5820 - mae: 76.9806 - val_loss: 41795.7930 - val_mae: 76.5574\n",
      "Epoch 426/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 41886.5820 - mae: 76.8322 - val_loss: 41609.0195 - val_mae: 76.4048\n",
      "Epoch 427/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 41452.8789 - mae: 76.5279 - val_loss: 41421.9961 - val_mae: 76.2511\n",
      "Epoch 428/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 41296.8398 - mae: 76.3326 - val_loss: 41238.5938 - val_mae: 76.0959\n",
      "Epoch 429/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 41149.7695 - mae: 76.2767 - val_loss: 41054.7500 - val_mae: 75.9477\n",
      "Epoch 430/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 40907.9609 - mae: 76.1026 - val_loss: 40871.8555 - val_mae: 75.7923\n",
      "Epoch 431/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 40917.5938 - mae: 76.0985 - val_loss: 40686.5312 - val_mae: 75.6415\n",
      "Epoch 432/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 40644.6484 - mae: 75.7981 - val_loss: 40501.9727 - val_mae: 75.4890\n",
      "Epoch 433/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 40326.3945 - mae: 75.5819 - val_loss: 40318.1797 - val_mae: 75.3353\n",
      "Epoch 434/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 39983.4023 - mae: 75.2986 - val_loss: 40133.3711 - val_mae: 75.1748\n",
      "Epoch 435/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 39971.7148 - mae: 75.3179 - val_loss: 39949.8398 - val_mae: 75.0208\n",
      "Epoch 436/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 40034.4023 - mae: 75.1908 - val_loss: 39767.6797 - val_mae: 74.8690\n",
      "Epoch 437/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 39835.5469 - mae: 75.0646 - val_loss: 39583.8945 - val_mae: 74.7200\n",
      "Epoch 438/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 39527.1562 - mae: 74.8791 - val_loss: 39403.3086 - val_mae: 74.5686\n",
      "Epoch 439/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 39214.0547 - mae: 74.6457 - val_loss: 39220.7148 - val_mae: 74.4181\n",
      "Epoch 440/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 39192.0508 - mae: 74.5536 - val_loss: 39038.1484 - val_mae: 74.2618\n",
      "Epoch 441/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 39030.8125 - mae: 74.4590 - val_loss: 38857.0117 - val_mae: 74.1125\n",
      "Epoch 442/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 38577.0156 - mae: 74.0373 - val_loss: 38674.7461 - val_mae: 73.9559\n",
      "Epoch 443/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 38633.6797 - mae: 74.0865 - val_loss: 38493.2422 - val_mae: 73.7995\n",
      "Epoch 444/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 38465.4609 - mae: 73.9300 - val_loss: 38311.2422 - val_mae: 73.6473\n",
      "Epoch 445/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 38343.0352 - mae: 73.7571 - val_loss: 38129.9062 - val_mae: 73.4917\n",
      "Epoch 446/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 38018.1211 - mae: 73.5591 - val_loss: 37948.6758 - val_mae: 73.3334\n",
      "Epoch 447/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 37687.0547 - mae: 73.3623 - val_loss: 37765.4492 - val_mae: 73.1745\n",
      "Epoch 448/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 37508.7812 - mae: 73.1748 - val_loss: 37584.2031 - val_mae: 73.0224\n",
      "Epoch 449/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 37539.7695 - mae: 73.2711 - val_loss: 37405.2578 - val_mae: 72.8622\n",
      "Epoch 450/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 37299.6523 - mae: 72.9893 - val_loss: 37226.8086 - val_mae: 72.7075\n",
      "Epoch 451/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 37081.2344 - mae: 72.8605 - val_loss: 37046.7969 - val_mae: 72.5486\n",
      "Epoch 452/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 37028.5000 - mae: 72.7442 - val_loss: 36868.3906 - val_mae: 72.3964\n",
      "Epoch 453/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 36757.3242 - mae: 72.5334 - val_loss: 36691.2031 - val_mae: 72.2442\n",
      "Epoch 454/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 36691.6094 - mae: 72.4666 - val_loss: 36509.7930 - val_mae: 72.0815\n",
      "Epoch 455/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 36480.7734 - mae: 72.2456 - val_loss: 36332.9805 - val_mae: 71.9289\n",
      "Epoch 456/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 36155.1172 - mae: 71.9223 - val_loss: 36152.9414 - val_mae: 71.7713\n",
      "Epoch 457/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 36034.0195 - mae: 71.8758 - val_loss: 35975.3125 - val_mae: 71.6139\n",
      "Epoch 458/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 35986.6680 - mae: 71.8641 - val_loss: 35794.7578 - val_mae: 71.4545\n",
      "Epoch 459/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 35592.1289 - mae: 71.4680 - val_loss: 35617.9375 - val_mae: 71.2986\n",
      "Epoch 460/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 35534.5391 - mae: 71.4762 - val_loss: 35441.4844 - val_mae: 71.1412\n",
      "Epoch 461/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 35257.2812 - mae: 71.1654 - val_loss: 35264.1953 - val_mae: 70.9827\n",
      "Epoch 462/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 35023.9883 - mae: 70.9080 - val_loss: 35087.6797 - val_mae: 70.8287\n",
      "Epoch 463/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 35194.6797 - mae: 71.1442 - val_loss: 34909.0703 - val_mae: 70.6646\n",
      "Epoch 464/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 34801.5117 - mae: 70.7967 - val_loss: 34733.3945 - val_mae: 70.5104\n",
      "Epoch 465/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 34575.5078 - mae: 70.4675 - val_loss: 34557.6094 - val_mae: 70.3555\n",
      "Epoch 466/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 34535.5234 - mae: 70.6256 - val_loss: 34383.4375 - val_mae: 70.1944\n",
      "Epoch 467/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 34573.7305 - mae: 70.5005 - val_loss: 34208.8008 - val_mae: 70.0346\n",
      "Epoch 468/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 34176.6758 - mae: 70.1667 - val_loss: 34033.3789 - val_mae: 69.8809\n",
      "Epoch 469/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 33893.4102 - mae: 69.8871 - val_loss: 33858.0625 - val_mae: 69.7250\n",
      "Epoch 470/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 33762.8867 - mae: 69.8402 - val_loss: 33683.4141 - val_mae: 69.5665\n",
      "Epoch 471/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 33593.1211 - mae: 69.5923 - val_loss: 33510.2266 - val_mae: 69.4013\n",
      "Epoch 472/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 33395.6094 - mae: 69.4404 - val_loss: 33336.7227 - val_mae: 69.2468\n",
      "Epoch 473/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 33195.6055 - mae: 69.2699 - val_loss: 33162.6523 - val_mae: 69.0892\n",
      "Epoch 474/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 33104.3828 - mae: 69.2257 - val_loss: 32989.4648 - val_mae: 68.9295\n",
      "Epoch 475/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 32748.1348 - mae: 68.8408 - val_loss: 32814.1602 - val_mae: 68.7687\n",
      "Epoch 476/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 32755.1777 - mae: 68.8736 - val_loss: 32639.4180 - val_mae: 68.6074\n",
      "Epoch 477/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 32667.2910 - mae: 68.7630 - val_loss: 32469.2461 - val_mae: 68.4484\n",
      "Epoch 478/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 32296.3750 - mae: 68.5849 - val_loss: 32296.7227 - val_mae: 68.2900\n",
      "Epoch 479/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 32405.5312 - mae: 68.5627 - val_loss: 32124.0664 - val_mae: 68.1264\n",
      "Epoch 480/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 31802.6250 - mae: 68.0879 - val_loss: 31953.7676 - val_mae: 67.9650\n",
      "Epoch 481/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 32057.7910 - mae: 68.2212 - val_loss: 31783.2324 - val_mae: 67.8081\n",
      "Epoch 482/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 31983.2031 - mae: 68.2377 - val_loss: 31611.9297 - val_mae: 67.6502\n",
      "Epoch 483/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 31399.2852 - mae: 67.5660 - val_loss: 31440.7168 - val_mae: 67.4878\n",
      "Epoch 484/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 31403.6445 - mae: 67.6452 - val_loss: 31270.1426 - val_mae: 67.3273\n",
      "Epoch 485/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 31236.8770 - mae: 67.5645 - val_loss: 31099.8652 - val_mae: 67.1661\n",
      "Epoch 486/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 31049.4043 - mae: 67.3058 - val_loss: 30930.5547 - val_mae: 67.0049\n",
      "Epoch 487/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 31002.6016 - mae: 67.3089 - val_loss: 30760.4414 - val_mae: 66.8426\n",
      "Epoch 488/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 30703.9082 - mae: 66.9121 - val_loss: 30592.2441 - val_mae: 66.6821\n",
      "Epoch 489/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 30637.5332 - mae: 66.9550 - val_loss: 30425.0859 - val_mae: 66.5213\n",
      "Epoch 490/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 30348.5645 - mae: 66.6475 - val_loss: 30255.8262 - val_mae: 66.3598\n",
      "Epoch 491/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 30184.5645 - mae: 66.4300 - val_loss: 30086.8926 - val_mae: 66.1969\n",
      "Epoch 492/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 30077.5645 - mae: 66.4175 - val_loss: 29919.5820 - val_mae: 66.0328\n",
      "Epoch 493/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 29716.4961 - mae: 66.0228 - val_loss: 29751.6348 - val_mae: 65.8724\n",
      "Epoch 494/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 29507.8457 - mae: 65.8393 - val_loss: 29584.0605 - val_mae: 65.7089\n",
      "Epoch 495/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 29596.3340 - mae: 65.9501 - val_loss: 29418.2383 - val_mae: 65.5459\n",
      "Epoch 496/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 29293.5703 - mae: 65.7022 - val_loss: 29251.3867 - val_mae: 65.3828\n",
      "Epoch 497/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 29188.2754 - mae: 65.5270 - val_loss: 29084.5000 - val_mae: 65.2206\n",
      "Epoch 498/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 28767.4414 - mae: 65.1212 - val_loss: 28918.7383 - val_mae: 65.0589\n",
      "Epoch 499/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28890.4512 - mae: 65.2495 - val_loss: 28753.5234 - val_mae: 64.8932\n",
      "Epoch 500/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28743.2793 - mae: 65.0354 - val_loss: 28588.4297 - val_mae: 64.7300\n",
      "Epoch 501/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28611.6445 - mae: 64.9288 - val_loss: 28423.8320 - val_mae: 64.5693\n",
      "Epoch 502/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28265.7695 - mae: 64.6500 - val_loss: 28258.5938 - val_mae: 64.4065\n",
      "Epoch 503/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28124.1895 - mae: 64.4210 - val_loss: 28095.8926 - val_mae: 64.2447\n",
      "Epoch 504/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 27928.7754 - mae: 64.2811 - val_loss: 27932.4883 - val_mae: 64.0759\n",
      "Epoch 505/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 28003.8281 - mae: 64.3821 - val_loss: 27769.8516 - val_mae: 63.9136\n",
      "Epoch 506/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 27834.2520 - mae: 64.2049 - val_loss: 27606.0605 - val_mae: 63.7582\n",
      "Epoch 507/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 27610.5547 - mae: 63.9798 - val_loss: 27443.6094 - val_mae: 63.5901\n",
      "Epoch 508/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 27349.2852 - mae: 63.6909 - val_loss: 27281.7715 - val_mae: 63.4241\n",
      "Epoch 509/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 27228.1836 - mae: 63.5426 - val_loss: 27121.3965 - val_mae: 63.2702\n",
      "Epoch 510/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 26970.2910 - mae: 63.3087 - val_loss: 26959.2441 - val_mae: 63.1017\n",
      "Epoch 511/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 26780.3105 - mae: 63.0235 - val_loss: 26798.0801 - val_mae: 62.9349\n",
      "Epoch 512/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 26658.8340 - mae: 63.0088 - val_loss: 26637.6191 - val_mae: 62.7667\n",
      "Epoch 513/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 26573.4512 - mae: 62.8959 - val_loss: 26476.9551 - val_mae: 62.6044\n",
      "Epoch 514/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 26366.0703 - mae: 62.6515 - val_loss: 26316.0664 - val_mae: 62.4395\n",
      "Epoch 515/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 26193.2090 - mae: 62.5658 - val_loss: 26156.2891 - val_mae: 62.2716\n",
      "Epoch 516/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 26001.6035 - mae: 62.2613 - val_loss: 25996.5117 - val_mae: 62.1060\n",
      "Epoch 517/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 25835.2910 - mae: 62.2267 - val_loss: 25837.2988 - val_mae: 61.9418\n",
      "Epoch 518/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 25633.3398 - mae: 61.8107 - val_loss: 25678.1680 - val_mae: 61.7737\n",
      "Epoch 519/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 25809.2969 - mae: 62.1257 - val_loss: 25521.2559 - val_mae: 61.6157\n",
      "Epoch 520/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 25439.2988 - mae: 61.7239 - val_loss: 25362.7949 - val_mae: 61.4446\n",
      "Epoch 521/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 25446.2598 - mae: 61.7230 - val_loss: 25205.4551 - val_mae: 61.2820\n",
      "Epoch 522/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 25217.9844 - mae: 61.4343 - val_loss: 25048.2480 - val_mae: 61.1157\n",
      "Epoch 523/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 24877.2246 - mae: 61.1534 - val_loss: 24892.2305 - val_mae: 60.9505\n",
      "Epoch 524/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 24860.9355 - mae: 61.1196 - val_loss: 24736.6934 - val_mae: 60.7901\n",
      "Epoch 525/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 24700.3691 - mae: 60.9007 - val_loss: 24581.0137 - val_mae: 60.6186\n",
      "Epoch 526/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 24695.4121 - mae: 60.9095 - val_loss: 24425.3906 - val_mae: 60.4507\n",
      "Epoch 527/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 24368.2539 - mae: 60.4325 - val_loss: 24269.7676 - val_mae: 60.2904\n",
      "Epoch 528/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 24218.7539 - mae: 60.3039 - val_loss: 24114.9473 - val_mae: 60.1200\n",
      "Epoch 529/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 24128.7969 - mae: 60.2940 - val_loss: 23961.7344 - val_mae: 59.9550\n",
      "Epoch 530/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 23785.6367 - mae: 59.9393 - val_loss: 23807.2188 - val_mae: 59.7866\n",
      "Epoch 531/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 23970.7461 - mae: 60.1143 - val_loss: 23653.0898 - val_mae: 59.6161\n",
      "Epoch 532/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 23406.0527 - mae: 59.5507 - val_loss: 23499.5547 - val_mae: 59.4522\n",
      "Epoch 533/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 23387.0137 - mae: 59.5356 - val_loss: 23347.3574 - val_mae: 59.2798\n",
      "Epoch 534/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 23203.4434 - mae: 59.2799 - val_loss: 23196.0938 - val_mae: 59.1132\n",
      "Epoch 535/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 23112.9668 - mae: 59.0323 - val_loss: 23043.8672 - val_mae: 58.9507\n",
      "Epoch 536/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22966.0801 - mae: 59.0099 - val_loss: 22892.7266 - val_mae: 58.7858\n",
      "Epoch 537/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22924.8535 - mae: 58.9596 - val_loss: 22741.5254 - val_mae: 58.6169\n",
      "Epoch 538/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22679.7422 - mae: 58.7193 - val_loss: 22590.3164 - val_mae: 58.4471\n",
      "Epoch 539/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22651.8770 - mae: 58.7809 - val_loss: 22439.9902 - val_mae: 58.2817\n",
      "Epoch 540/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 22416.4336 - mae: 58.3757 - val_loss: 22290.1445 - val_mae: 58.1137\n",
      "Epoch 541/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22522.3027 - mae: 58.5035 - val_loss: 22140.3652 - val_mae: 57.9478\n",
      "Epoch 542/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 22287.4844 - mae: 58.2599 - val_loss: 21991.4434 - val_mae: 57.7794\n",
      "Epoch 543/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21859.8535 - mae: 57.7928 - val_loss: 21843.4375 - val_mae: 57.6104\n",
      "Epoch 544/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 22108.4688 - mae: 58.1174 - val_loss: 21693.5957 - val_mae: 57.4427\n",
      "Epoch 545/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21602.6895 - mae: 57.4173 - val_loss: 21546.2910 - val_mae: 57.2736\n",
      "Epoch 546/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 21413.0820 - mae: 57.2864 - val_loss: 21398.7363 - val_mae: 57.1094\n",
      "Epoch 547/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21214.7324 - mae: 56.9901 - val_loss: 21252.3047 - val_mae: 56.9370\n",
      "Epoch 548/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 21116.0566 - mae: 57.0397 - val_loss: 21106.2891 - val_mae: 56.7655\n",
      "Epoch 549/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20963.6875 - mae: 56.8195 - val_loss: 20960.0488 - val_mae: 56.6010\n",
      "Epoch 550/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20923.1309 - mae: 56.7457 - val_loss: 20814.5352 - val_mae: 56.4296\n",
      "Epoch 551/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20661.7812 - mae: 56.4351 - val_loss: 20668.5312 - val_mae: 56.2571\n",
      "Epoch 552/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20680.6250 - mae: 56.4529 - val_loss: 20522.9902 - val_mae: 56.0895\n",
      "Epoch 553/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20673.4180 - mae: 56.4172 - val_loss: 20378.9824 - val_mae: 55.9226\n",
      "Epoch 554/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20361.6934 - mae: 56.1166 - val_loss: 20237.5762 - val_mae: 55.7551\n",
      "Epoch 555/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 20118.0254 - mae: 55.7121 - val_loss: 20094.7188 - val_mae: 55.5857\n",
      "Epoch 556/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 20081.1719 - mae: 55.7102 - val_loss: 19951.8301 - val_mae: 55.4217\n",
      "Epoch 557/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 19878.1250 - mae: 55.4170 - val_loss: 19809.9316 - val_mae: 55.2512\n",
      "Epoch 558/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 19811.1152 - mae: 55.3734 - val_loss: 19668.5156 - val_mae: 55.0843\n",
      "Epoch 559/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 19532.0039 - mae: 55.1187 - val_loss: 19526.6855 - val_mae: 54.9132\n",
      "Epoch 560/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 19375.3965 - mae: 54.8318 - val_loss: 19385.9277 - val_mae: 54.7431\n",
      "Epoch 561/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 19363.3496 - mae: 54.8848 - val_loss: 19245.7949 - val_mae: 54.5763\n",
      "Epoch 562/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 19346.9863 - mae: 54.8549 - val_loss: 19105.4160 - val_mae: 54.4038\n",
      "Epoch 563/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 19114.1660 - mae: 54.6279 - val_loss: 18964.2402 - val_mae: 54.2288\n",
      "Epoch 564/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 18838.9180 - mae: 54.1994 - val_loss: 18825.9258 - val_mae: 54.0642\n",
      "Epoch 565/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 18911.2070 - mae: 54.2547 - val_loss: 18686.7539 - val_mae: 53.8949\n",
      "Epoch 566/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 18588.7344 - mae: 53.9888 - val_loss: 18547.4062 - val_mae: 53.7238\n",
      "Epoch 567/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 18641.1133 - mae: 53.9960 - val_loss: 18409.3262 - val_mae: 53.5529\n",
      "Epoch 568/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 18413.7871 - mae: 53.6819 - val_loss: 18271.3594 - val_mae: 53.3853\n",
      "Epoch 569/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 18363.3223 - mae: 53.5336 - val_loss: 18134.0098 - val_mae: 53.2114\n",
      "Epoch 570/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 18148.2246 - mae: 53.4594 - val_loss: 17996.6016 - val_mae: 53.0399\n",
      "Epoch 571/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17949.2539 - mae: 53.0994 - val_loss: 17860.0391 - val_mae: 52.8741\n",
      "Epoch 572/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17748.9727 - mae: 52.9186 - val_loss: 17726.0371 - val_mae: 52.7026\n",
      "Epoch 573/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17924.5469 - mae: 53.1030 - val_loss: 17590.2910 - val_mae: 52.5308\n",
      "Epoch 574/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17640.3086 - mae: 52.6238 - val_loss: 17456.2188 - val_mae: 52.3610\n",
      "Epoch 575/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17630.2324 - mae: 52.7825 - val_loss: 17320.8496 - val_mae: 52.1898\n",
      "Epoch 576/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17259.2480 - mae: 52.2549 - val_loss: 17189.1211 - val_mae: 52.0290\n",
      "Epoch 577/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17082.3750 - mae: 51.9751 - val_loss: 17055.1719 - val_mae: 51.8515\n",
      "Epoch 578/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16932.3320 - mae: 51.8676 - val_loss: 16922.5098 - val_mae: 51.6841\n",
      "Epoch 579/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16912.5312 - mae: 51.7466 - val_loss: 16791.9336 - val_mae: 51.5130\n",
      "Epoch 580/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16718.0898 - mae: 51.5506 - val_loss: 16659.5781 - val_mae: 51.3392\n",
      "Epoch 581/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16694.7637 - mae: 51.5349 - val_loss: 16528.3711 - val_mae: 51.1658\n",
      "Epoch 582/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16413.4102 - mae: 51.1145 - val_loss: 16395.9863 - val_mae: 50.9895\n",
      "Epoch 583/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16469.7754 - mae: 51.2324 - val_loss: 16266.1367 - val_mae: 50.8224\n",
      "Epoch 584/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16296.7910 - mae: 51.0103 - val_loss: 16135.2744 - val_mae: 50.6500\n",
      "Epoch 585/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 16265.1260 - mae: 50.9358 - val_loss: 16005.3184 - val_mae: 50.4815\n",
      "Epoch 586/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 15941.0049 - mae: 50.4570 - val_loss: 15876.1064 - val_mae: 50.3139\n",
      "Epoch 587/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 15921.4980 - mae: 50.5497 - val_loss: 15747.1572 - val_mae: 50.1411\n",
      "Epoch 588/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 15609.2129 - mae: 50.0322 - val_loss: 15620.9600 - val_mae: 49.9656\n",
      "Epoch 589/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 15644.8955 - mae: 50.0499 - val_loss: 15495.3447 - val_mae: 49.8000\n",
      "Epoch 590/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 15487.9170 - mae: 49.9438 - val_loss: 15367.5869 - val_mae: 49.6274\n",
      "Epoch 591/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 15277.0938 - mae: 49.6057 - val_loss: 15241.0781 - val_mae: 49.4559\n",
      "Epoch 592/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 15245.5303 - mae: 49.5514 - val_loss: 15115.2471 - val_mae: 49.2872\n",
      "Epoch 593/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 15307.1367 - mae: 49.6128 - val_loss: 14988.2100 - val_mae: 49.1121\n",
      "Epoch 594/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14866.1670 - mae: 48.9935 - val_loss: 14864.1494 - val_mae: 48.9415\n",
      "Epoch 595/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14802.6807 - mae: 48.9356 - val_loss: 14740.3037 - val_mae: 48.7716\n",
      "Epoch 596/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14864.7793 - mae: 49.0286 - val_loss: 14616.2471 - val_mae: 48.5995\n",
      "Epoch 597/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14641.3008 - mae: 48.7858 - val_loss: 14491.2363 - val_mae: 48.4262\n",
      "Epoch 598/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14483.3408 - mae: 48.4966 - val_loss: 14369.8047 - val_mae: 48.2561\n",
      "Epoch 599/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14551.0947 - mae: 48.6330 - val_loss: 14247.9375 - val_mae: 48.0880\n",
      "Epoch 600/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14256.7588 - mae: 48.2628 - val_loss: 14125.5508 - val_mae: 47.9108\n",
      "Epoch 601/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14425.9336 - mae: 48.4797 - val_loss: 14004.7412 - val_mae: 47.7423\n",
      "Epoch 602/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 13828.3818 - mae: 47.6493 - val_loss: 13882.0625 - val_mae: 47.5662\n",
      "Epoch 603/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 13967.8662 - mae: 47.8279 - val_loss: 13762.8369 - val_mae: 47.3998\n",
      "Epoch 604/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 13781.1270 - mae: 47.5086 - val_loss: 13640.9053 - val_mae: 47.2223\n",
      "Epoch 605/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 13473.8086 - mae: 47.0325 - val_loss: 13522.3887 - val_mae: 47.0497\n",
      "Epoch 606/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 13607.0557 - mae: 47.2112 - val_loss: 13403.2070 - val_mae: 46.8785\n",
      "Epoch 607/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 13451.7422 - mae: 47.0598 - val_loss: 13286.0459 - val_mae: 46.7079\n",
      "Epoch 608/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 13467.5488 - mae: 47.1008 - val_loss: 13168.4941 - val_mae: 46.5387\n",
      "Epoch 609/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 13253.0371 - mae: 46.8035 - val_loss: 13051.0166 - val_mae: 46.3668\n",
      "Epoch 610/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 13021.4785 - mae: 46.3706 - val_loss: 12934.4346 - val_mae: 46.1998\n",
      "Epoch 611/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12889.8799 - mae: 46.1987 - val_loss: 12817.8232 - val_mae: 46.0222\n",
      "Epoch 612/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12783.8008 - mae: 46.1269 - val_loss: 12701.8652 - val_mae: 45.8511\n",
      "Epoch 613/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12674.5469 - mae: 45.8342 - val_loss: 12586.6045 - val_mae: 45.6768\n",
      "Epoch 614/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12614.5645 - mae: 45.7722 - val_loss: 12472.1602 - val_mae: 45.5063\n",
      "Epoch 615/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 12319.3525 - mae: 45.3740 - val_loss: 12359.2930 - val_mae: 45.3321\n",
      "Epoch 616/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12527.5576 - mae: 45.5969 - val_loss: 12246.2607 - val_mae: 45.1662\n",
      "Epoch 617/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12305.0459 - mae: 45.3322 - val_loss: 12133.3076 - val_mae: 44.9941\n",
      "Epoch 618/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12150.3467 - mae: 45.1081 - val_loss: 12018.8633 - val_mae: 44.8208\n",
      "Epoch 619/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12244.7070 - mae: 45.1532 - val_loss: 11907.5693 - val_mae: 44.6536\n",
      "Epoch 620/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 12050.8555 - mae: 44.9243 - val_loss: 11794.8945 - val_mae: 44.4752\n",
      "Epoch 621/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11855.1816 - mae: 44.7286 - val_loss: 11683.9688 - val_mae: 44.3016\n",
      "Epoch 622/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11697.9336 - mae: 44.2787 - val_loss: 11574.5762 - val_mae: 44.1316\n",
      "Epoch 623/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11709.4102 - mae: 44.4547 - val_loss: 11464.1768 - val_mae: 43.9600\n",
      "Epoch 624/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11566.9258 - mae: 44.1282 - val_loss: 11353.7168 - val_mae: 43.7861\n",
      "Epoch 625/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11402.6943 - mae: 43.9188 - val_loss: 11246.5654 - val_mae: 43.6157\n",
      "Epoch 626/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11341.2168 - mae: 43.7922 - val_loss: 11137.8086 - val_mae: 43.4442\n",
      "Epoch 627/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11144.9307 - mae: 43.5454 - val_loss: 11029.7305 - val_mae: 43.2719\n",
      "Epoch 628/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11216.3047 - mae: 43.6880 - val_loss: 10922.5244 - val_mae: 43.1007\n",
      "Epoch 629/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 11173.2910 - mae: 43.5216 - val_loss: 10815.8838 - val_mae: 42.9328\n",
      "Epoch 630/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 10839.0283 - mae: 43.0257 - val_loss: 10709.8799 - val_mae: 42.7602\n",
      "Epoch 631/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 10740.1572 - mae: 42.8989 - val_loss: 10603.9805 - val_mae: 42.5803\n",
      "Epoch 632/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 10619.8076 - mae: 42.6771 - val_loss: 10499.1885 - val_mae: 42.4099\n",
      "Epoch 633/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 10534.0264 - mae: 42.5419 - val_loss: 10394.4814 - val_mae: 42.2376\n",
      "Epoch 634/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 10335.7812 - mae: 42.2436 - val_loss: 10292.1338 - val_mae: 42.0680\n",
      "Epoch 635/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 10369.6572 - mae: 42.1974 - val_loss: 10186.8906 - val_mae: 41.8898\n",
      "Epoch 636/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 10184.1162 - mae: 41.8960 - val_loss: 10084.8213 - val_mae: 41.7193\n",
      "Epoch 637/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 10253.4854 - mae: 42.0483 - val_loss: 9981.7559 - val_mae: 41.5519\n",
      "Epoch 638/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 10068.3164 - mae: 41.8127 - val_loss: 9879.8037 - val_mae: 41.3762\n",
      "Epoch 639/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 9891.3193 - mae: 41.4202 - val_loss: 9780.3145 - val_mae: 41.2103\n",
      "Epoch 640/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9695.7051 - mae: 41.1038 - val_loss: 9680.5859 - val_mae: 41.0394\n",
      "Epoch 641/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9591.2861 - mae: 40.9032 - val_loss: 9578.8906 - val_mae: 40.8654\n",
      "Epoch 642/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9690.6758 - mae: 40.9753 - val_loss: 9480.3174 - val_mae: 40.6936\n",
      "Epoch 643/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9481.7900 - mae: 40.7559 - val_loss: 9381.7412 - val_mae: 40.5208\n",
      "Epoch 644/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9334.7002 - mae: 40.5372 - val_loss: 9285.3213 - val_mae: 40.3558\n",
      "Epoch 645/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9255.4766 - mae: 40.3908 - val_loss: 9186.4229 - val_mae: 40.1813\n",
      "Epoch 646/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9224.6436 - mae: 40.3735 - val_loss: 9087.3242 - val_mae: 40.0095\n",
      "Epoch 647/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 9020.4912 - mae: 39.9024 - val_loss: 8991.0205 - val_mae: 39.8353\n",
      "Epoch 648/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 9060.7490 - mae: 40.0416 - val_loss: 8896.9746 - val_mae: 39.6688\n",
      "Epoch 649/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 8892.0127 - mae: 39.7274 - val_loss: 8800.9492 - val_mae: 39.4956\n",
      "Epoch 650/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 8809.9414 - mae: 39.5182 - val_loss: 8706.3047 - val_mae: 39.3256\n",
      "Epoch 651/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 8706.1094 - mae: 39.3159 - val_loss: 8610.6113 - val_mae: 39.1530\n",
      "Epoch 652/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 8656.0010 - mae: 39.3255 - val_loss: 8517.2441 - val_mae: 38.9831\n",
      "Epoch 653/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 8508.1387 - mae: 38.9459 - val_loss: 8424.5332 - val_mae: 38.8110\n",
      "Epoch 654/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 8421.3135 - mae: 38.8890 - val_loss: 8332.9570 - val_mae: 38.6462\n",
      "Epoch 655/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 8395.0557 - mae: 38.7499 - val_loss: 8238.7500 - val_mae: 38.4687\n",
      "Epoch 656/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 8293.1914 - mae: 38.5534 - val_loss: 8146.9199 - val_mae: 38.3013\n",
      "Epoch 657/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 8135.5845 - mae: 38.3153 - val_loss: 8058.4902 - val_mae: 38.1357\n",
      "Epoch 658/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 8101.2998 - mae: 38.2148 - val_loss: 7968.1104 - val_mae: 37.9640\n",
      "Epoch 659/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7986.4473 - mae: 37.9938 - val_loss: 7876.5010 - val_mae: 37.7912\n",
      "Epoch 660/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7913.7202 - mae: 37.8244 - val_loss: 7786.1372 - val_mae: 37.6197\n",
      "Epoch 661/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7923.0171 - mae: 37.9458 - val_loss: 7700.3477 - val_mae: 37.4527\n",
      "Epoch 662/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7850.3081 - mae: 37.6913 - val_loss: 7610.9282 - val_mae: 37.2823\n",
      "Epoch 663/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7786.5547 - mae: 37.6191 - val_loss: 7521.4946 - val_mae: 37.1058\n",
      "Epoch 664/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7540.8120 - mae: 37.0975 - val_loss: 7434.2241 - val_mae: 36.9375\n",
      "Epoch 665/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7560.0635 - mae: 37.1615 - val_loss: 7348.8208 - val_mae: 36.7719\n",
      "Epoch 666/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7454.1748 - mae: 37.0018 - val_loss: 7261.8882 - val_mae: 36.5999\n",
      "Epoch 667/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7333.6489 - mae: 36.7253 - val_loss: 7176.5684 - val_mae: 36.4291\n",
      "Epoch 668/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7346.1382 - mae: 36.8206 - val_loss: 7093.8901 - val_mae: 36.2651\n",
      "Epoch 669/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7086.4546 - mae: 36.2360 - val_loss: 7008.2007 - val_mae: 36.0944\n",
      "Epoch 670/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 7134.9136 - mae: 36.2614 - val_loss: 6924.6865 - val_mae: 35.9284\n",
      "Epoch 671/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6937.1440 - mae: 35.9079 - val_loss: 6839.9800 - val_mae: 35.7529\n",
      "Epoch 672/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6881.9028 - mae: 35.7410 - val_loss: 6758.8057 - val_mae: 35.5878\n",
      "Epoch 673/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6872.6211 - mae: 35.7520 - val_loss: 6677.2881 - val_mae: 35.4175\n",
      "Epoch 674/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6775.8374 - mae: 35.5679 - val_loss: 6595.2788 - val_mae: 35.2486\n",
      "Epoch 675/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6694.5269 - mae: 35.3859 - val_loss: 6514.5581 - val_mae: 35.0799\n",
      "Epoch 676/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6578.7974 - mae: 35.2396 - val_loss: 6433.7651 - val_mae: 34.9080\n",
      "Epoch 677/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6605.5903 - mae: 35.1155 - val_loss: 6354.5469 - val_mae: 34.7405\n",
      "Epoch 678/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6466.1279 - mae: 34.9840 - val_loss: 6275.3984 - val_mae: 34.5727\n",
      "Epoch 679/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6279.9575 - mae: 34.4509 - val_loss: 6196.3257 - val_mae: 34.3982\n",
      "Epoch 680/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6318.0059 - mae: 34.7032 - val_loss: 6119.2749 - val_mae: 34.2342\n",
      "Epoch 681/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6233.4297 - mae: 34.3618 - val_loss: 6041.6138 - val_mae: 34.0670\n",
      "Epoch 682/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6039.3774 - mae: 34.0822 - val_loss: 5964.2700 - val_mae: 33.8969\n",
      "Epoch 683/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 6129.4653 - mae: 34.2638 - val_loss: 5888.4565 - val_mae: 33.7359\n",
      "Epoch 684/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5972.8652 - mae: 33.7403 - val_loss: 5812.1885 - val_mae: 33.5648\n",
      "Epoch 685/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5996.3247 - mae: 33.8790 - val_loss: 5736.3340 - val_mae: 33.3976\n",
      "Epoch 686/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5728.1763 - mae: 33.2545 - val_loss: 5662.7041 - val_mae: 33.2323\n",
      "Epoch 687/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5798.5259 - mae: 33.5091 - val_loss: 5588.9126 - val_mae: 33.0645\n",
      "Epoch 688/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5576.3857 - mae: 32.9352 - val_loss: 5515.2485 - val_mae: 32.8958\n",
      "Epoch 689/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5500.1978 - mae: 32.7681 - val_loss: 5443.1079 - val_mae: 32.7300\n",
      "Epoch 690/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 5467.5117 - mae: 32.6991 - val_loss: 5371.6064 - val_mae: 32.5652\n",
      "Epoch 691/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5467.3276 - mae: 32.6246 - val_loss: 5300.2319 - val_mae: 32.4042\n",
      "Epoch 692/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5443.5527 - mae: 32.5824 - val_loss: 5228.2808 - val_mae: 32.2352\n",
      "Epoch 693/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5368.0513 - mae: 32.5145 - val_loss: 5158.1499 - val_mae: 32.0697\n",
      "Epoch 694/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5300.1035 - mae: 32.3574 - val_loss: 5087.6362 - val_mae: 31.9006\n",
      "Epoch 695/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 5242.7974 - mae: 32.1783 - val_loss: 5017.9175 - val_mae: 31.7335\n",
      "Epoch 696/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 5088.5210 - mae: 31.8063 - val_loss: 4949.7246 - val_mae: 31.5670\n",
      "Epoch 697/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 5129.7026 - mae: 31.8684 - val_loss: 4880.4956 - val_mae: 31.4059\n",
      "Epoch 698/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 4901.8433 - mae: 31.3026 - val_loss: 4813.6226 - val_mae: 31.2418\n",
      "Epoch 699/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4823.1562 - mae: 31.1336 - val_loss: 4746.7026 - val_mae: 31.0751\n",
      "Epoch 700/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4850.4253 - mae: 31.2311 - val_loss: 4680.4385 - val_mae: 30.9172\n",
      "Epoch 701/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4876.9512 - mae: 31.2782 - val_loss: 4614.4004 - val_mae: 30.7483\n",
      "Epoch 702/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4716.7349 - mae: 30.8887 - val_loss: 4548.9800 - val_mae: 30.5841\n",
      "Epoch 703/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4743.0576 - mae: 30.9855 - val_loss: 4483.9468 - val_mae: 30.4222\n",
      "Epoch 704/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4598.3579 - mae: 30.5630 - val_loss: 4420.0215 - val_mae: 30.2615\n",
      "Epoch 705/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4548.2373 - mae: 30.3968 - val_loss: 4356.0532 - val_mae: 30.0952\n",
      "Epoch 706/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4462.4824 - mae: 30.1854 - val_loss: 4293.3257 - val_mae: 29.9314\n",
      "Epoch 707/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4277.8848 - mae: 29.6472 - val_loss: 4230.2632 - val_mae: 29.7672\n",
      "Epoch 708/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 4362.4038 - mae: 29.8794 - val_loss: 4169.2842 - val_mae: 29.6059\n",
      "Epoch 709/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4276.4326 - mae: 29.6560 - val_loss: 4108.0200 - val_mae: 29.4466\n",
      "Epoch 710/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 4161.1963 - mae: 29.4283 - val_loss: 4047.4597 - val_mae: 29.2781\n",
      "Epoch 711/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4291.8711 - mae: 29.6635 - val_loss: 3987.3105 - val_mae: 29.1238\n",
      "Epoch 712/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4036.2473 - mae: 29.0436 - val_loss: 3926.5518 - val_mae: 28.9541\n",
      "Epoch 713/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4064.7588 - mae: 29.0002 - val_loss: 3868.0830 - val_mae: 28.7920\n",
      "Epoch 714/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 4019.0801 - mae: 28.9787 - val_loss: 3808.6643 - val_mae: 28.6291\n",
      "Epoch 715/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3934.4636 - mae: 28.6597 - val_loss: 3751.4380 - val_mae: 28.4697\n",
      "Epoch 716/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3951.0107 - mae: 28.7449 - val_loss: 3693.8967 - val_mae: 28.3128\n",
      "Epoch 717/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3837.6299 - mae: 28.6404 - val_loss: 3637.0510 - val_mae: 28.1505\n",
      "Epoch 718/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3825.0818 - mae: 28.4208 - val_loss: 3579.9531 - val_mae: 27.9896\n",
      "Epoch 719/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3697.1257 - mae: 28.1598 - val_loss: 3524.1799 - val_mae: 27.8277\n",
      "Epoch 720/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3700.5950 - mae: 28.1426 - val_loss: 3469.4844 - val_mae: 27.6739\n",
      "Epoch 721/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3635.6250 - mae: 27.8960 - val_loss: 3413.4089 - val_mae: 27.5074\n",
      "Epoch 722/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3528.1897 - mae: 27.5289 - val_loss: 3360.5088 - val_mae: 27.3494\n",
      "Epoch 723/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3476.0813 - mae: 27.4715 - val_loss: 3306.5964 - val_mae: 27.1938\n",
      "Epoch 724/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3389.4551 - mae: 27.1159 - val_loss: 3255.1665 - val_mae: 27.0397\n",
      "Epoch 725/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3396.2856 - mae: 27.2025 - val_loss: 3201.8430 - val_mae: 26.8777\n",
      "Epoch 726/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3228.7532 - mae: 26.6968 - val_loss: 3150.4729 - val_mae: 26.7192\n",
      "Epoch 727/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3274.9844 - mae: 26.8082 - val_loss: 3099.3999 - val_mae: 26.5583\n",
      "Epoch 728/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3325.6936 - mae: 27.0116 - val_loss: 3048.0837 - val_mae: 26.4029\n",
      "Epoch 729/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3159.6218 - mae: 26.4215 - val_loss: 2997.3518 - val_mae: 26.2454\n",
      "Epoch 730/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 3103.4810 - mae: 26.3579 - val_loss: 2948.1611 - val_mae: 26.0867\n",
      "Epoch 731/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 3114.5103 - mae: 26.3274 - val_loss: 2898.5063 - val_mae: 25.9312\n",
      "Epoch 732/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2992.2271 - mae: 26.0071 - val_loss: 2850.6902 - val_mae: 25.7754\n",
      "Epoch 733/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2982.3064 - mae: 25.9109 - val_loss: 2802.1836 - val_mae: 25.6201\n",
      "Epoch 734/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2942.4382 - mae: 25.6325 - val_loss: 2755.1711 - val_mae: 25.4652\n",
      "Epoch 735/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2858.7285 - mae: 25.4483 - val_loss: 2707.8491 - val_mae: 25.3092\n",
      "Epoch 736/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 2877.9470 - mae: 25.5691 - val_loss: 2662.7803 - val_mae: 25.1581\n",
      "Epoch 737/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2871.4309 - mae: 25.6061 - val_loss: 2614.7695 - val_mae: 24.9936\n",
      "Epoch 738/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2816.5093 - mae: 25.3086 - val_loss: 2569.8647 - val_mae: 24.8435\n",
      "Epoch 739/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2660.5144 - mae: 24.7963 - val_loss: 2524.8259 - val_mae: 24.6917\n",
      "Epoch 740/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2652.1360 - mae: 24.6864 - val_loss: 2481.9487 - val_mae: 24.5424\n",
      "Epoch 741/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2531.0525 - mae: 24.3111 - val_loss: 2437.3511 - val_mae: 24.3848\n",
      "Epoch 742/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2597.6250 - mae: 24.4756 - val_loss: 2396.5288 - val_mae: 24.2455\n",
      "Epoch 743/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2534.4897 - mae: 24.2663 - val_loss: 2352.9766 - val_mae: 24.0907\n",
      "Epoch 744/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2474.1147 - mae: 24.1298 - val_loss: 2311.5535 - val_mae: 23.9399\n",
      "Epoch 745/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2471.2151 - mae: 24.0597 - val_loss: 2269.8594 - val_mae: 23.7878\n",
      "Epoch 746/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2441.3508 - mae: 24.0435 - val_loss: 2228.3323 - val_mae: 23.6352\n",
      "Epoch 747/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2336.2007 - mae: 23.6334 - val_loss: 2189.2263 - val_mae: 23.4907\n",
      "Epoch 748/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2437.2791 - mae: 24.0219 - val_loss: 2147.0940 - val_mae: 23.3341\n",
      "Epoch 749/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2186.2012 - mae: 23.0325 - val_loss: 2108.8740 - val_mae: 23.1937\n",
      "Epoch 750/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2206.0505 - mae: 23.1818 - val_loss: 2069.7900 - val_mae: 23.0466\n",
      "Epoch 751/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2209.7822 - mae: 23.1013 - val_loss: 2031.5394 - val_mae: 22.8941\n",
      "Epoch 752/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2226.7473 - mae: 23.0624 - val_loss: 1994.3251 - val_mae: 22.7546\n",
      "Epoch 753/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2200.3391 - mae: 23.0303 - val_loss: 1955.5564 - val_mae: 22.6035\n",
      "Epoch 754/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 2209.1567 - mae: 23.0536 - val_loss: 1919.1865 - val_mae: 22.4574\n",
      "Epoch 755/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2116.5300 - mae: 22.7251 - val_loss: 1881.7693 - val_mae: 22.3034\n",
      "Epoch 756/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 2056.6467 - mae: 22.5647 - val_loss: 1845.0173 - val_mae: 22.1544\n",
      "Epoch 757/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1982.1270 - mae: 22.2031 - val_loss: 1810.1162 - val_mae: 22.0110\n",
      "Epoch 758/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1996.3042 - mae: 22.3963 - val_loss: 1774.7819 - val_mae: 21.8625\n",
      "Epoch 759/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1970.4382 - mae: 22.1338 - val_loss: 1739.5663 - val_mae: 21.7184\n",
      "Epoch 760/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1928.3485 - mae: 22.0527 - val_loss: 1705.8339 - val_mae: 21.5726\n",
      "Epoch 761/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1813.0962 - mae: 21.5480 - val_loss: 1672.1770 - val_mae: 21.4277\n",
      "Epoch 762/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1842.9141 - mae: 21.5936 - val_loss: 1639.5455 - val_mae: 21.2912\n",
      "Epoch 763/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1802.2659 - mae: 21.5190 - val_loss: 1607.1604 - val_mae: 21.1475\n",
      "Epoch 764/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1797.6016 - mae: 21.3636 - val_loss: 1574.1846 - val_mae: 21.0042\n",
      "Epoch 765/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1645.6818 - mae: 20.8239 - val_loss: 1543.5955 - val_mae: 20.8655\n",
      "Epoch 766/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1693.8420 - mae: 20.9108 - val_loss: 1512.9686 - val_mae: 20.7218\n",
      "Epoch 767/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1652.6748 - mae: 20.7698 - val_loss: 1482.6223 - val_mae: 20.5831\n",
      "Epoch 768/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1631.4843 - mae: 20.6579 - val_loss: 1452.4932 - val_mae: 20.4439\n",
      "Epoch 769/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1607.3160 - mae: 20.5318 - val_loss: 1423.5128 - val_mae: 20.3082\n",
      "Epoch 770/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1617.4564 - mae: 20.4284 - val_loss: 1394.1737 - val_mae: 20.1741\n",
      "Epoch 771/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1569.3486 - mae: 20.4373 - val_loss: 1365.1876 - val_mae: 20.0304\n",
      "Epoch 772/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1534.2684 - mae: 20.0887 - val_loss: 1337.1270 - val_mae: 19.8961\n",
      "Epoch 773/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1535.7871 - mae: 20.1248 - val_loss: 1309.3135 - val_mae: 19.7570\n",
      "Epoch 774/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1513.1233 - mae: 20.1681 - val_loss: 1281.6331 - val_mae: 19.6166\n",
      "Epoch 775/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1472.8561 - mae: 19.8627 - val_loss: 1254.7566 - val_mae: 19.4861\n",
      "Epoch 776/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 1403.0636 - mae: 19.5235 - val_loss: 1228.7628 - val_mae: 19.3522\n",
      "Epoch 777/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1393.7770 - mae: 19.4719 - val_loss: 1202.6252 - val_mae: 19.2138\n",
      "Epoch 778/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1363.5439 - mae: 19.2939 - val_loss: 1177.2728 - val_mae: 19.0816\n",
      "Epoch 779/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1404.5657 - mae: 19.4889 - val_loss: 1152.3018 - val_mae: 18.9469\n",
      "Epoch 780/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1337.7988 - mae: 19.1217 - val_loss: 1127.2292 - val_mae: 18.8129\n",
      "Epoch 781/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1267.2748 - mae: 18.8638 - val_loss: 1102.8188 - val_mae: 18.6802\n",
      "Epoch 782/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1220.1998 - mae: 18.5486 - val_loss: 1080.0319 - val_mae: 18.5518\n",
      "Epoch 783/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1268.9420 - mae: 18.8154 - val_loss: 1056.4525 - val_mae: 18.4201\n",
      "Epoch 784/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1298.1687 - mae: 18.9582 - val_loss: 1033.2170 - val_mae: 18.2896\n",
      "Epoch 785/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1197.1432 - mae: 18.3487 - val_loss: 1010.9451 - val_mae: 18.1618\n",
      "Epoch 786/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1220.7220 - mae: 18.4860 - val_loss: 988.5192 - val_mae: 18.0328\n",
      "Epoch 787/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1203.1415 - mae: 18.3270 - val_loss: 966.7390 - val_mae: 17.9041\n",
      "Epoch 788/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1160.2057 - mae: 18.0979 - val_loss: 945.8220 - val_mae: 17.7807\n",
      "Epoch 789/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1153.2028 - mae: 18.1088 - val_loss: 924.1912 - val_mae: 17.6574\n",
      "Epoch 790/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1087.8970 - mae: 17.6706 - val_loss: 903.8589 - val_mae: 17.5269\n",
      "Epoch 791/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1066.8105 - mae: 17.5449 - val_loss: 884.5747 - val_mae: 17.4025\n",
      "Epoch 792/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1049.1945 - mae: 17.5323 - val_loss: 864.6354 - val_mae: 17.2763\n",
      "Epoch 793/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1055.8427 - mae: 17.4899 - val_loss: 846.1342 - val_mae: 17.1600\n",
      "Epoch 794/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1065.8214 - mae: 17.5778 - val_loss: 826.5403 - val_mae: 17.0326\n",
      "Epoch 795/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 1036.2753 - mae: 17.4955 - val_loss: 807.7147 - val_mae: 16.9073\n",
      "Epoch 796/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 988.5457 - mae: 17.0226 - val_loss: 789.7463 - val_mae: 16.7892\n",
      "Epoch 797/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 937.3787 - mae: 16.8176 - val_loss: 772.9193 - val_mae: 16.6757\n",
      "Epoch 798/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 1005.2797 - mae: 17.2752 - val_loss: 755.3216 - val_mae: 16.5558\n",
      "Epoch 799/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 954.2089 - mae: 16.8188 - val_loss: 737.7281 - val_mae: 16.4356\n",
      "Epoch 800/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 942.4062 - mae: 16.7970 - val_loss: 721.4667 - val_mae: 16.3162\n",
      "Epoch 801/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 957.2532 - mae: 16.9290 - val_loss: 704.6606 - val_mae: 16.1956\n",
      "Epoch 802/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 906.7601 - mae: 16.6569 - val_loss: 688.9099 - val_mae: 16.0810\n",
      "Epoch 803/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 916.9500 - mae: 16.4612 - val_loss: 672.8574 - val_mae: 15.9577\n",
      "Epoch 804/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 897.7089 - mae: 16.3923 - val_loss: 657.2741 - val_mae: 15.8423\n",
      "Epoch 805/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 804.2598 - mae: 15.9645 - val_loss: 642.3565 - val_mae: 15.7277\n",
      "Epoch 806/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 886.0600 - mae: 16.4199 - val_loss: 627.6201 - val_mae: 15.6153\n",
      "Epoch 807/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 861.0308 - mae: 16.2488 - val_loss: 613.1592 - val_mae: 15.5039\n",
      "Epoch 808/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 816.5439 - mae: 15.9140 - val_loss: 599.2763 - val_mae: 15.3878\n",
      "Epoch 809/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 793.8184 - mae: 15.8399 - val_loss: 585.5995 - val_mae: 15.2763\n",
      "Epoch 810/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 773.2335 - mae: 15.5328 - val_loss: 572.4557 - val_mae: 15.1676\n",
      "Epoch 811/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 771.6703 - mae: 15.7010 - val_loss: 559.3912 - val_mae: 15.0611\n",
      "Epoch 812/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 798.7137 - mae: 15.8908 - val_loss: 546.5350 - val_mae: 14.9471\n",
      "Epoch 813/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 787.3317 - mae: 15.7245 - val_loss: 534.0826 - val_mae: 14.8394\n",
      "Epoch 814/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 723.0704 - mae: 15.2000 - val_loss: 521.9788 - val_mae: 14.7306\n",
      "Epoch 815/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 738.3544 - mae: 15.2906 - val_loss: 510.4864 - val_mae: 14.6269\n",
      "Epoch 816/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 711.8730 - mae: 15.1337 - val_loss: 499.0815 - val_mae: 14.5213\n",
      "Epoch 817/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 686.7443 - mae: 14.9578 - val_loss: 488.1855 - val_mae: 14.4201\n",
      "Epoch 818/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 718.8351 - mae: 15.2271 - val_loss: 477.3108 - val_mae: 14.3137\n",
      "Epoch 819/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 649.6534 - mae: 14.6985 - val_loss: 466.4585 - val_mae: 14.2107\n",
      "Epoch 820/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 677.6460 - mae: 14.8564 - val_loss: 456.6245 - val_mae: 14.1129\n",
      "Epoch 821/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 693.8521 - mae: 14.9864 - val_loss: 446.9167 - val_mae: 14.0156\n",
      "Epoch 822/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 635.8732 - mae: 14.5703 - val_loss: 436.9495 - val_mae: 13.9129\n",
      "Epoch 823/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 639.0078 - mae: 14.6358 - val_loss: 427.7364 - val_mae: 13.8178\n",
      "Epoch 824/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 666.8932 - mae: 14.8068 - val_loss: 418.9940 - val_mae: 13.7237\n",
      "Epoch 825/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 642.8350 - mae: 14.6205 - val_loss: 409.6233 - val_mae: 13.6275\n",
      "Epoch 826/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 609.5508 - mae: 14.3485 - val_loss: 401.1335 - val_mae: 13.5302\n",
      "Epoch 827/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 620.7733 - mae: 14.4303 - val_loss: 392.9334 - val_mae: 13.4381\n",
      "Epoch 828/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 609.0659 - mae: 14.4177 - val_loss: 384.7744 - val_mae: 13.3451\n",
      "Epoch 829/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 630.0941 - mae: 14.5525 - val_loss: 376.6737 - val_mae: 13.2505\n",
      "Epoch 830/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 597.4626 - mae: 14.3361 - val_loss: 368.8325 - val_mae: 13.1580\n",
      "Epoch 831/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 560.6064 - mae: 14.0434 - val_loss: 361.5721 - val_mae: 13.0698\n",
      "Epoch 832/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 601.4938 - mae: 14.3876 - val_loss: 353.9625 - val_mae: 12.9738\n",
      "Epoch 833/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 557.8781 - mae: 13.9618 - val_loss: 346.8294 - val_mae: 12.8803\n",
      "Epoch 834/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 560.3241 - mae: 14.0178 - val_loss: 340.7381 - val_mae: 12.8028\n",
      "Epoch 835/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 539.6176 - mae: 13.9291 - val_loss: 334.2557 - val_mae: 12.7187\n",
      "Epoch 836/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 561.7410 - mae: 14.0178 - val_loss: 327.3911 - val_mae: 12.6247\n",
      "Epoch 837/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 540.5973 - mae: 13.7852 - val_loss: 321.3526 - val_mae: 12.5404\n",
      "Epoch 838/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 536.9114 - mae: 13.8344 - val_loss: 315.3676 - val_mae: 12.4571\n",
      "Epoch 839/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 508.0338 - mae: 13.6134 - val_loss: 309.7816 - val_mae: 12.3771\n",
      "Epoch 840/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 516.5889 - mae: 13.7370 - val_loss: 304.4724 - val_mae: 12.3016\n",
      "Epoch 841/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 497.2215 - mae: 13.5486 - val_loss: 299.2033 - val_mae: 12.2230\n",
      "Epoch 842/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 493.2648 - mae: 13.4264 - val_loss: 294.0109 - val_mae: 12.1410\n",
      "Epoch 843/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 486.6086 - mae: 13.4094 - val_loss: 289.2334 - val_mae: 12.0677\n",
      "Epoch 844/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 545.3098 - mae: 13.8662 - val_loss: 284.2812 - val_mae: 11.9893\n",
      "Epoch 845/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 485.3086 - mae: 13.4234 - val_loss: 279.4153 - val_mae: 11.9084\n",
      "Epoch 846/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 486.3161 - mae: 13.4352 - val_loss: 275.2361 - val_mae: 11.8378\n",
      "Epoch 847/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 509.3372 - mae: 13.5191 - val_loss: 270.7580 - val_mae: 11.7652\n",
      "Epoch 848/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 497.2003 - mae: 13.4409 - val_loss: 266.5062 - val_mae: 11.6906\n",
      "Epoch 849/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 452.5668 - mae: 13.1182 - val_loss: 262.5504 - val_mae: 11.6219\n",
      "Epoch 850/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 472.2284 - mae: 13.3538 - val_loss: 258.5155 - val_mae: 11.5447\n",
      "Epoch 851/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 476.9885 - mae: 13.3642 - val_loss: 254.8903 - val_mae: 11.4784\n",
      "Epoch 852/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 489.4809 - mae: 13.5110 - val_loss: 251.4944 - val_mae: 11.4170\n",
      "Epoch 853/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 444.3936 - mae: 12.9851 - val_loss: 248.3353 - val_mae: 11.3502\n",
      "Epoch 854/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 455.8126 - mae: 13.0398 - val_loss: 245.6784 - val_mae: 11.2968\n",
      "Epoch 855/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 477.7881 - mae: 13.2843 - val_loss: 242.5246 - val_mae: 11.2355\n",
      "Epoch 856/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 466.9490 - mae: 13.2394 - val_loss: 239.9191 - val_mae: 11.1777\n",
      "Epoch 857/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 468.4926 - mae: 13.3296 - val_loss: 236.8070 - val_mae: 11.1162\n",
      "Epoch 858/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 438.6696 - mae: 13.0625 - val_loss: 234.3130 - val_mae: 11.0609\n",
      "Epoch 859/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 449.2162 - mae: 13.0791 - val_loss: 231.3459 - val_mae: 10.9930\n",
      "Epoch 860/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 463.7223 - mae: 13.3118 - val_loss: 228.4567 - val_mae: 10.9239\n",
      "Epoch 861/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 472.4426 - mae: 13.3298 - val_loss: 225.7305 - val_mae: 10.8626\n",
      "Epoch 862/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 460.8603 - mae: 13.1297 - val_loss: 222.8582 - val_mae: 10.7974\n",
      "Epoch 863/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 451.8147 - mae: 13.2233 - val_loss: 220.7942 - val_mae: 10.7407\n",
      "Epoch 864/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 451.7196 - mae: 13.0918 - val_loss: 218.2925 - val_mae: 10.6751\n",
      "Epoch 865/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 455.5685 - mae: 13.1085 - val_loss: 216.3205 - val_mae: 10.6210\n",
      "Epoch 866/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 433.8755 - mae: 12.9949 - val_loss: 214.1145 - val_mae: 10.5650\n",
      "Epoch 867/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 455.2527 - mae: 13.1373 - val_loss: 212.1124 - val_mae: 10.5101\n",
      "Epoch 868/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 417.9941 - mae: 12.8348 - val_loss: 210.1493 - val_mae: 10.4548\n",
      "Epoch 869/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 444.0130 - mae: 13.0528 - val_loss: 208.5766 - val_mae: 10.4075\n",
      "Epoch 870/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 433.8984 - mae: 12.9902 - val_loss: 206.9500 - val_mae: 10.3581\n",
      "Epoch 871/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 424.7368 - mae: 12.8536 - val_loss: 205.3524 - val_mae: 10.3068\n",
      "Epoch 872/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 435.6569 - mae: 12.8886 - val_loss: 203.6311 - val_mae: 10.2567\n",
      "Epoch 873/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 442.1398 - mae: 13.0653 - val_loss: 202.2405 - val_mae: 10.2101\n",
      "Epoch 874/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 426.9215 - mae: 12.9171 - val_loss: 201.1952 - val_mae: 10.1715\n",
      "Epoch 875/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 432.1902 - mae: 12.8716 - val_loss: 199.2600 - val_mae: 10.1065\n",
      "Epoch 876/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 437.2035 - mae: 13.1203 - val_loss: 198.1944 - val_mae: 10.0695\n",
      "Epoch 877/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 423.0972 - mae: 12.8918 - val_loss: 197.0858 - val_mae: 10.0300\n",
      "Epoch 878/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 425.2607 - mae: 12.8963 - val_loss: 195.8952 - val_mae: 9.9854\n",
      "Epoch 879/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 429.6088 - mae: 13.0392 - val_loss: 194.8166 - val_mae: 9.9443\n",
      "Epoch 880/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 429.4093 - mae: 12.9972 - val_loss: 193.6439 - val_mae: 9.8983\n",
      "Epoch 881/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 417.1740 - mae: 12.8489 - val_loss: 192.7850 - val_mae: 9.8682\n",
      "Epoch 882/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 409.9207 - mae: 12.7520 - val_loss: 192.0546 - val_mae: 9.8311\n",
      "Epoch 883/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 432.0566 - mae: 13.0282 - val_loss: 191.2317 - val_mae: 9.7989\n",
      "Epoch 884/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 406.9999 - mae: 12.8092 - val_loss: 190.4103 - val_mae: 9.7664\n",
      "Epoch 885/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 400.3924 - mae: 12.7065 - val_loss: 189.8375 - val_mae: 9.7423\n",
      "Epoch 886/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 424.5029 - mae: 12.9998 - val_loss: 189.1901 - val_mae: 9.7151\n",
      "Epoch 887/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 448.5232 - mae: 13.1154 - val_loss: 188.2327 - val_mae: 9.6781\n",
      "Epoch 888/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 433.6354 - mae: 13.0277 - val_loss: 187.6704 - val_mae: 9.6526\n",
      "Epoch 889/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 416.3587 - mae: 12.8462 - val_loss: 186.9266 - val_mae: 9.6253\n",
      "Epoch 890/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 413.0770 - mae: 12.7731 - val_loss: 186.5597 - val_mae: 9.6045\n",
      "Epoch 891/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 417.9148 - mae: 12.9124 - val_loss: 185.8673 - val_mae: 9.5800\n",
      "Epoch 892/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 399.5954 - mae: 12.7076 - val_loss: 185.4433 - val_mae: 9.5663\n",
      "Epoch 893/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 397.4798 - mae: 12.6939 - val_loss: 184.9751 - val_mae: 9.5447\n",
      "Epoch 894/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 407.0674 - mae: 12.6709 - val_loss: 184.6801 - val_mae: 9.5300\n",
      "Epoch 895/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 421.5281 - mae: 12.8988 - val_loss: 184.2873 - val_mae: 9.5110\n",
      "Epoch 896/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 412.4394 - mae: 12.8342 - val_loss: 184.0320 - val_mae: 9.4984\n",
      "Epoch 897/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 428.0190 - mae: 12.9698 - val_loss: 183.7324 - val_mae: 9.4835\n",
      "Epoch 898/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 430.4078 - mae: 12.9478 - val_loss: 183.3445 - val_mae: 9.4721\n",
      "Epoch 899/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 405.0943 - mae: 12.6868 - val_loss: 182.8817 - val_mae: 9.4524\n",
      "Epoch 900/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 420.6349 - mae: 12.8633 - val_loss: 182.5788 - val_mae: 9.4359\n",
      "Epoch 901/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 438.4702 - mae: 13.0131 - val_loss: 182.1446 - val_mae: 9.4190\n",
      "Epoch 902/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 427.6975 - mae: 12.9162 - val_loss: 181.7848 - val_mae: 9.4067\n",
      "Epoch 903/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 400.1822 - mae: 12.7249 - val_loss: 181.7341 - val_mae: 9.3968\n",
      "Epoch 904/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 399.4776 - mae: 12.7019 - val_loss: 181.3417 - val_mae: 9.3904\n",
      "Epoch 905/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 414.2517 - mae: 12.7540 - val_loss: 181.1529 - val_mae: 9.3804\n",
      "Epoch 906/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 435.9962 - mae: 13.0389 - val_loss: 181.0190 - val_mae: 9.3738\n",
      "Epoch 907/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 418.1697 - mae: 12.7987 - val_loss: 181.0784 - val_mae: 9.3779\n",
      "Epoch 908/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 413.7707 - mae: 12.8074 - val_loss: 180.6931 - val_mae: 9.3637\n",
      "Epoch 909/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 419.3362 - mae: 12.9199 - val_loss: 180.5560 - val_mae: 9.3575\n",
      "Epoch 910/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 425.1335 - mae: 12.9338 - val_loss: 180.3015 - val_mae: 9.3462\n",
      "Epoch 911/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 412.8818 - mae: 12.7943 - val_loss: 180.0605 - val_mae: 9.3416\n",
      "Epoch 912/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 402.1410 - mae: 12.6476 - val_loss: 180.0231 - val_mae: 9.3345\n",
      "Epoch 913/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 423.2118 - mae: 12.9538 - val_loss: 179.7380 - val_mae: 9.3254\n",
      "Epoch 914/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 403.7931 - mae: 12.7955 - val_loss: 179.6599 - val_mae: 9.3228\n",
      "Epoch 915/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 403.3854 - mae: 12.7035 - val_loss: 179.6037 - val_mae: 9.3174\n",
      "Epoch 916/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 406.0955 - mae: 12.6851 - val_loss: 179.5377 - val_mae: 9.3158\n",
      "Epoch 917/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 422.5494 - mae: 12.8786 - val_loss: 179.5757 - val_mae: 9.3189\n",
      "Epoch 918/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 406.6515 - mae: 12.6852 - val_loss: 179.4831 - val_mae: 9.3164\n",
      "Epoch 919/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 404.1881 - mae: 12.7496 - val_loss: 179.4346 - val_mae: 9.3148\n",
      "Epoch 920/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 427.2271 - mae: 12.8891 - val_loss: 179.4566 - val_mae: 9.3150\n",
      "Epoch 921/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 390.2674 - mae: 12.6180 - val_loss: 179.4350 - val_mae: 9.3177\n",
      "Epoch 922/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 422.4547 - mae: 12.9950 - val_loss: 179.2040 - val_mae: 9.3065\n",
      "Epoch 923/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 438.6364 - mae: 13.0167 - val_loss: 178.9935 - val_mae: 9.2984\n",
      "Epoch 924/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 425.7300 - mae: 13.0999 - val_loss: 179.0046 - val_mae: 9.2978\n",
      "Epoch 925/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 421.1874 - mae: 12.8412 - val_loss: 178.9058 - val_mae: 9.2973\n",
      "Epoch 926/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 430.1124 - mae: 12.9488 - val_loss: 178.6808 - val_mae: 9.2867\n",
      "Epoch 927/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 421.7384 - mae: 12.9766 - val_loss: 178.6479 - val_mae: 9.2821\n",
      "Epoch 928/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 401.1679 - mae: 12.7184 - val_loss: 178.5733 - val_mae: 9.2847\n",
      "Epoch 929/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 411.7429 - mae: 12.8133 - val_loss: 178.6178 - val_mae: 9.2857\n",
      "Epoch 930/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 418.7633 - mae: 12.8604 - val_loss: 178.7424 - val_mae: 9.2840\n",
      "Epoch 931/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 438.0081 - mae: 12.9864 - val_loss: 178.6332 - val_mae: 9.2843\n",
      "Epoch 932/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 411.0947 - mae: 12.8252 - val_loss: 178.5304 - val_mae: 9.2788\n",
      "Epoch 933/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 411.9168 - mae: 12.8034 - val_loss: 178.6253 - val_mae: 9.2814\n",
      "Epoch 934/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 421.9514 - mae: 12.8114 - val_loss: 178.4931 - val_mae: 9.2735\n",
      "Epoch 935/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 410.5844 - mae: 12.8164 - val_loss: 178.2251 - val_mae: 9.2667\n",
      "Epoch 936/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 422.0470 - mae: 12.9045 - val_loss: 178.4093 - val_mae: 9.2746\n",
      "Epoch 937/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 434.1410 - mae: 12.9609 - val_loss: 178.3507 - val_mae: 9.2710\n",
      "Epoch 938/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 407.4965 - mae: 12.7774 - val_loss: 178.3060 - val_mae: 9.2771\n",
      "Epoch 939/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 422.7942 - mae: 12.8441 - val_loss: 178.3882 - val_mae: 9.2740\n",
      "Epoch 940/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 418.2269 - mae: 12.8281 - val_loss: 178.3121 - val_mae: 9.2688\n",
      "Epoch 941/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 388.9659 - mae: 12.5638 - val_loss: 178.1774 - val_mae: 9.2698\n",
      "Epoch 942/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 405.1294 - mae: 12.7367 - val_loss: 178.1402 - val_mae: 9.2629\n",
      "Epoch 943/1000\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 380.9928 - mae: 12.5507 - val_loss: 178.1643 - val_mae: 9.2672\n",
      "Epoch 944/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 408.4703 - mae: 12.7586 - val_loss: 178.1006 - val_mae: 9.2649\n",
      "Epoch 945/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 395.4888 - mae: 12.6607 - val_loss: 178.1984 - val_mae: 9.2663\n",
      "Epoch 946/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 402.2834 - mae: 12.7502 - val_loss: 178.2402 - val_mae: 9.2716\n",
      "Epoch 947/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 405.6547 - mae: 12.7793 - val_loss: 178.2353 - val_mae: 9.2690\n",
      "Epoch 948/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 423.5264 - mae: 12.9713 - val_loss: 178.2185 - val_mae: 9.2672\n",
      "Epoch 949/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 418.2798 - mae: 12.8858 - val_loss: 178.0827 - val_mae: 9.2655\n",
      "Epoch 950/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 413.8631 - mae: 12.7997 - val_loss: 178.0938 - val_mae: 9.2621\n",
      "Epoch 951/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 426.1299 - mae: 12.8913 - val_loss: 178.0702 - val_mae: 9.2629\n",
      "Epoch 952/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 409.9843 - mae: 12.7529 - val_loss: 178.1901 - val_mae: 9.2663\n",
      "Epoch 953/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 416.8815 - mae: 12.8851 - val_loss: 178.2418 - val_mae: 9.2720\n",
      "Epoch 954/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 406.8779 - mae: 12.6972 - val_loss: 178.1684 - val_mae: 9.2667\n",
      "Epoch 955/1000\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 439.4232 - mae: 13.0174 - val_loss: 178.0423 - val_mae: 9.2608\n",
      "Epoch 956/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 417.6400 - mae: 12.7704 - val_loss: 177.8883 - val_mae: 9.2574\n",
      "Epoch 957/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 416.7399 - mae: 12.8392 - val_loss: 177.9809 - val_mae: 9.2592\n",
      "Epoch 958/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 406.8725 - mae: 12.7107 - val_loss: 177.9136 - val_mae: 9.2555\n",
      "Epoch 959/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 397.3362 - mae: 12.5975 - val_loss: 177.7577 - val_mae: 9.2519\n",
      "Epoch 960/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 410.7335 - mae: 12.7987 - val_loss: 177.6294 - val_mae: 9.2504\n",
      "Epoch 961/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 421.9580 - mae: 12.9220 - val_loss: 177.5868 - val_mae: 9.2520\n",
      "Epoch 962/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 400.0699 - mae: 12.6269 - val_loss: 177.6994 - val_mae: 9.2556\n",
      "Epoch 963/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 418.2557 - mae: 12.8531 - val_loss: 177.6876 - val_mae: 9.2491\n",
      "Epoch 964/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 404.3863 - mae: 12.6788 - val_loss: 177.6959 - val_mae: 9.2491\n",
      "Epoch 965/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 414.5134 - mae: 12.7480 - val_loss: 177.7239 - val_mae: 9.2524\n",
      "Epoch 966/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 394.9740 - mae: 12.6331 - val_loss: 177.8038 - val_mae: 9.2533\n",
      "Epoch 967/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 406.7039 - mae: 12.8217 - val_loss: 177.6710 - val_mae: 9.2498\n",
      "Epoch 968/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 397.8149 - mae: 12.6894 - val_loss: 177.6801 - val_mae: 9.2505\n",
      "Epoch 969/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 409.7028 - mae: 12.7577 - val_loss: 177.7412 - val_mae: 9.2500\n",
      "Epoch 970/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 417.7546 - mae: 12.8967 - val_loss: 177.6113 - val_mae: 9.2475\n",
      "Epoch 971/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 416.9820 - mae: 12.8280 - val_loss: 177.5533 - val_mae: 9.2504\n",
      "Epoch 972/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 420.6665 - mae: 12.9689 - val_loss: 177.5754 - val_mae: 9.2484\n",
      "Epoch 973/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 424.6643 - mae: 12.9065 - val_loss: 177.5580 - val_mae: 9.2488\n",
      "Epoch 974/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 417.8326 - mae: 12.8118 - val_loss: 177.5520 - val_mae: 9.2451\n",
      "Epoch 975/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 408.9238 - mae: 12.8265 - val_loss: 177.3436 - val_mae: 9.2434\n",
      "Epoch 976/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 400.5111 - mae: 12.6549 - val_loss: 177.4873 - val_mae: 9.2458\n",
      "Epoch 977/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 444.0606 - mae: 13.1247 - val_loss: 177.5793 - val_mae: 9.2470\n",
      "Epoch 978/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 413.7389 - mae: 12.7400 - val_loss: 177.5157 - val_mae: 9.2506\n",
      "Epoch 979/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 423.2299 - mae: 12.9307 - val_loss: 177.5468 - val_mae: 9.2480\n",
      "Epoch 980/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 405.4028 - mae: 12.7169 - val_loss: 177.4780 - val_mae: 9.2436\n",
      "Epoch 981/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 411.1986 - mae: 12.7985 - val_loss: 177.2187 - val_mae: 9.2328\n",
      "Epoch 982/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 420.6893 - mae: 12.8833 - val_loss: 177.1981 - val_mae: 9.2338\n",
      "Epoch 983/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 418.4015 - mae: 12.8715 - val_loss: 177.2153 - val_mae: 9.2330\n",
      "Epoch 984/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 432.5449 - mae: 13.0717 - val_loss: 177.2188 - val_mae: 9.2321\n",
      "Epoch 985/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 415.5046 - mae: 12.8690 - val_loss: 177.1218 - val_mae: 9.2319\n",
      "Epoch 986/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 401.3288 - mae: 12.6434 - val_loss: 177.1933 - val_mae: 9.2373\n",
      "Epoch 987/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 404.2030 - mae: 12.7777 - val_loss: 177.2673 - val_mae: 9.2416\n",
      "Epoch 988/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 421.7970 - mae: 12.9018 - val_loss: 177.3737 - val_mae: 9.2470\n",
      "Epoch 989/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 414.7856 - mae: 12.7558 - val_loss: 177.4311 - val_mae: 9.2433\n",
      "Epoch 990/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 398.2780 - mae: 12.7235 - val_loss: 177.4446 - val_mae: 9.2451\n",
      "Epoch 991/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 399.1889 - mae: 12.6857 - val_loss: 177.4517 - val_mae: 9.2433\n",
      "Epoch 992/1000\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 427.4185 - mae: 12.9103 - val_loss: 177.1984 - val_mae: 9.2381\n",
      "Epoch 993/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 392.2206 - mae: 12.6232 - val_loss: 177.3141 - val_mae: 9.2380\n",
      "Epoch 994/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 425.5607 - mae: 12.8192 - val_loss: 177.1906 - val_mae: 9.2382\n",
      "Epoch 995/1000\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 407.7327 - mae: 12.7708 - val_loss: 177.3225 - val_mae: 9.2419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28485e217c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(X_train, y_train, \n",
    "          epochs=1000, \n",
    "          batch_size=32, \n",
    "          validation_split=0.2,\n",
    "          callbacks=[early_stopping]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55cba81f-c562-4d7d-93c4-4e5d966f9e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 2ms/step\n",
      "MAE per output variable: [16.43397203 17.14617711 16.49291552 17.64283885  7.87863013  0.24691809\n",
      "  1.4762638   6.59444571  0.18660482]\n",
      "MSE per output variable: [3.45547930e+02 3.82255740e+02 3.50146991e+02 4.10507296e+02\n",
      " 9.72270643e+01 2.02524539e-01 3.46021232e+00 5.79124177e+01\n",
      " 4.84902210e-02]\n"
     ]
    }
   ],
   "source": [
    "# 7. 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 평가\n",
    "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "print(f\"MAE per output variable: {mae}\")\n",
    "print(f\"MSE per output variable: {mse}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
