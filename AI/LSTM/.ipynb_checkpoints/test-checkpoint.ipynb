{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type float).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 126\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# 7. 부트스트랩 학습 실행\u001b[39;00m\n\u001b[0;32m    125\u001b[0m n_bootstrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m--> 126\u001b[0m models, results \u001b[38;5;241m=\u001b[39m \u001b[43mbootstrap_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_fully_transformer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_bootstrap\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# 8. 예측 및 평가\u001b[39;00m\n\u001b[0;32m    133\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m ensemble_predict(models, X_test)\n",
      "Cell \u001b[1;32mIn[1], line 113\u001b[0m, in \u001b[0;36mbootstrap_training\u001b[1;34m(X, y, create_model_fn, n_bootstrap, epochs, batch_size, validation_split)\u001b[0m\n\u001b[0;32m    111\u001b[0m X_bootstrap, y_bootstrap \u001b[38;5;241m=\u001b[39m X[indices], y[indices]\n\u001b[0;32m    112\u001b[0m model \u001b[38;5;241m=\u001b[39m create_model_fn()\n\u001b[1;32m--> 113\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_bootstrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_bootstrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[0;32m    116\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mmin\u001b[39m(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\limul\\.conda\\envs\\tf-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\limul\\.conda\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type float)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, MultiHeadAttention, LayerNormalization, Add, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 데이터 로드 및 전처리\n",
    "pollutants_path = '../../NewData/Weekly_Air_Pollutants.csv'\n",
    "temperature_path = '../../ClimateDataTeam/climate_data/merged_weekly_avg_temp.csv'\n",
    "target_path = pollutants_path\n",
    "\n",
    "# 데이터 로드\n",
    "pollutants_df = pd.read_csv(target_path)\n",
    "temperature_df = pd.read_csv(temperature_path)\n",
    "\n",
    "# datetime 변환 및 병합\n",
    "pollutants_df['datetime'] = pd.to_datetime(pollutants_df['datetime'])\n",
    "temperature_df['datetime'] = pd.to_datetime(temperature_df['datetime'])\n",
    "merged_df = pd.merge(pollutants_df, temperature_df, on='datetime', how='inner')\n",
    "\n",
    "# 날짜 정보 추가\n",
    "merged_df['year'] = merged_df['datetime'].dt.year\n",
    "merged_df['month'] = merged_df['datetime'].dt.month\n",
    "merged_df['week'] = merged_df['datetime'].dt.isocalendar().week\n",
    "merged_df = merged_df.drop(columns=['datetime'])  # datetime 제거\n",
    "\n",
    "# 주기적 특징 추가 함수\n",
    "def add_periodic_features(df, period_column, max_period):\n",
    "    df[f\"{period_column}_sin\"] = np.sin(2 * np.pi * df[period_column] / max_period)\n",
    "    df[f\"{period_column}_cos\"] = np.cos(2 * np.pi * df[period_column] / max_period)\n",
    "    return df\n",
    "\n",
    "# 주기적 특징 추가\n",
    "merged_df = add_periodic_features(merged_df, \"week\", 52)\n",
    "merged_df = add_periodic_features(merged_df, \"month\", 12)\n",
    "\n",
    "# 출력 및 입력 변수 설정\n",
    "pollutants_column = pollutants_df.columns.tolist()[1:]  # datetime 제거\n",
    "climate_column = temperature_df.columns.tolist()[1:]\n",
    "X = merged_df.drop(columns=pollutants_column[:])  # 입력 변수 (주기적 특징 포함)\n",
    "y = merged_df[climate_column[:]]  # 출력 변수\n",
    "\n",
    "# 2. 시계열 윈도우 생성 함수\n",
    "def create_time_series_features(X, y, lag):\n",
    "    X_features, y_labels = [], []\n",
    "    for i in range(len(X) - lag):\n",
    "        X_features.append(X.iloc[i:i+lag].values)\n",
    "        y_labels.append(y.iloc[i+lag].values)\n",
    "    return np.array(X_features), np.array(y_labels)\n",
    "\n",
    "# 시계열 윈도우 생성\n",
    "lag = 30\n",
    "X_ts, y_ts = create_time_series_features(X, y, lag)\n",
    "\n",
    "# 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ts, y_ts, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터 타입 변환\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# 3. Transformer 블록 정의\n",
    "def transformer_block(x, num_heads, key_dim, ff_dim, dropout_rate):\n",
    "    # Multi-Head Attention  \n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    out1 = Add()([x, attn_output])  # Residual Connection\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "\n",
    "    # Feed-Forward Network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(out1)  # FFN의 출력 차원\n",
    "    ffn_output = Dense(x.shape[-1])(ffn_output)  # 입력 차원으로 다시 매핑\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    out2 = Add()([out1, ffn_output])  # Residual Connection\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "\n",
    "    return out2\n",
    "\n",
    "\n",
    "# 4. Transformer 모델 생성\n",
    "def create_fully_transformer_model(input_shape, output_dim, num_heads=4, key_dim=32, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Transformer Blocks\n",
    "    x = transformer_block(inputs, num_heads, key_dim, ff_dim, dropout_rate)\n",
    "    x = transformer_block(x, num_heads, key_dim, ff_dim, dropout_rate)\n",
    "    x = transformer_block(x, num_heads, key_dim, ff_dim, dropout_rate)\n",
    "\n",
    "    # Pooling and Dense Layers\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(output_dim)(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "# 5. 부트스트랩 학습 함수\n",
    "def bootstrap_training(X, y, create_model_fn, n_bootstrap=5, epochs=50, batch_size=32, validation_split=0.2):\n",
    "    models = []\n",
    "    results = []\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    for i in range(n_bootstrap):\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        \n",
    "        # 데이터 타입 강제 변환\n",
    "        X_bootstrap = X[indices].astype('float32')\n",
    "        y_bootstrap = y[indices].astype('float32')\n",
    "        \n",
    "        # 모델 생성 및 학습\n",
    "        model = create_model_fn()\n",
    "        history = model.fit(X_bootstrap, y_bootstrap, epochs=epochs, batch_size=batch_size, \n",
    "                            validation_split=validation_split, callbacks=[early_stopping], verbose=1)\n",
    "        models.append(model)\n",
    "        results.append(min(history.history['val_loss']))\n",
    "    return models, results\n",
    "\n",
    "# 6. 앙상블 예측 함수\n",
    "def ensemble_predict(models, X_test):\n",
    "    predictions = [model.predict(X_test) for model in models]\n",
    "    return np.mean(predictions, axis=0)\n",
    "\n",
    "# 7. 부트스트랩 학습 실행\n",
    "n_bootstrap = 5\n",
    "models, results = bootstrap_training(\n",
    "    X_train, y_train, \n",
    "    lambda: create_fully_transformer_model(input_shape=(lag, X_train.shape[2]), output_dim=y_train.shape[1]),\n",
    "    n_bootstrap=n_bootstrap\n",
    ")\n",
    "\n",
    "# 8. 예측 및 평가\n",
    "y_pred = ensemble_predict(models, X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "print(\"Validation Loss from Bootstrap Samples:\", results)\n",
    "print(\"Mean Validation Loss:\", np.mean(results))\n",
    "print(\"MAE per output variable:\", mae)\n",
    "print(\"MSE per output variable:\", mse)\n",
    "\n",
    "# 9. 시각화\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(range(len(y_test)), y_test[:, 0], label=\"Original Data\", color=\"blue\", linestyle=\"--\")\n",
    "plt.plot(range(len(y_pred)), y_pred[:, 0], label=\"Predicted Data\", color=\"red\", alpha=0.8)\n",
    "plt.title(\"Comparison of Actual vs Predicted Data\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
