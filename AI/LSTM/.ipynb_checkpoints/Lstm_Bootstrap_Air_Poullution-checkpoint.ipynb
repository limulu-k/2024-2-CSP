{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ef4a1-6953-48b6-8e77-5640a1cc2ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, MultiHeadAttention, LayerNormalization, Add, Conv1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df31a18-9069-4308-bc61-4057c08c2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "# 파일 경로\n",
    "\n",
    "pollutants_path = '../../NewData/Weekly_Air_Pollutants.csv'\n",
    "greenhouse_gas_path = '../../NewData/Weekly_Greenhouse_Gas.csv'\n",
    "population_path = '../../NewData/Weekly_Population.csv'\n",
    "power_usage_path = '../../NewData/Weekly_Power_Usage.csv'\n",
    "temperature_path = '../../ClimateDataTeam/climate_data/merged_weekly_avg_temp.csv'\n",
    "target_path = population_path\n",
    "\n",
    "# 데이터 로드\n",
    "pollutants_df = pd.read_csv(target_path)\n",
    "temperature_df = pd.read_csv(temperature_path)\n",
    "\n",
    "# 속성값 정의\n",
    "pollutants_column = pollutants_df.columns.tolist()\n",
    "climate_column = temperature_df.columns.tolist()\n",
    "\n",
    "# datetime 컬럼 변환 및 병합\n",
    "pollutants_df['datetime'] = pd.to_datetime(pollutants_df['datetime'])\n",
    "temperature_df['datetime'] = pd.to_datetime(temperature_df['datetime'])\n",
    "merged_df = pd.merge(pollutants_df, temperature_df, on='datetime', how='inner')\n",
    "\n",
    "# datetime 형변환 및 제거\n",
    "merged_df['year'] = merged_df['datetime'].dt.year\n",
    "merged_df['month'] = merged_df['datetime'].dt.month\n",
    "merged_df['week'] = merged_df['datetime'].dt.isocalendar().week\n",
    "merged_df = merged_df.drop(columns=['datetime'])  # datetime 제거\n",
    "\n",
    "# pollutants_column에 날짜데이터 추가\n",
    "pollutants_column.append('year')\n",
    "pollutants_column.append('month')\n",
    "pollutants_column.append('week')\n",
    "pollutants_column = pollutants_column[1:]\n",
    "climate_column = climate_column[1:]\n",
    "\n",
    "print(f\"pollutants_column : {pollutants_column}\")\n",
    "print(f\"climate_column : {climate_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfa6420-f874-47c7-b7be-53a9d6c6cdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X와 y 분리\n",
    "X = merged_df.drop(columns=pollutants_column[:])  # datetime 및 출력 변수 제외\n",
    "y = merged_df[climate_column[:]]  # 출력 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9c86a5-3c20-4b4d-83ff-23f58f880054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 시계열 윈도우 생성 함수\n",
    "def create_time_series_features(X, y, lag):\n",
    "    X_features, y_labels = [], []\n",
    "    for i in range(len(X) - lag):\n",
    "        X_features.append(X.iloc[i:i+lag].values)\n",
    "        y_labels.append(y.iloc[i+lag].values)\n",
    "    return np.array(X_features), np.array(y_labels)\n",
    "\n",
    "# 시계열 윈도우 생성\n",
    "lag = 30  # 과거 30주 데이터를 사용\n",
    "X_ts, y_ts = create_time_series_features(X, y, lag)\n",
    "\n",
    "# 데이터 확인\n",
    "print(f\"X_ts shape: {X_ts.shape}, y_ts shape: {y_ts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f81db-f142-48ad-a9e6-7cbf39353fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Transformer 블록 정의\n",
    "def transformer_block(x, num_heads, key_dim, ff_dim, dropout_rate):\n",
    "    # Multi-Head Attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(x, x)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    out1 = Add()([x, attn_output])\n",
    "    out1 = LayerNormalization(epsilon=1e-6)(out1)\n",
    "\n",
    "    # Feed-forward Network\n",
    "    ffn_output = Dense(ff_dim, activation=\"relu\")(out1)  # ff_dim이 out1의 feature 크기와 동일하게 설정\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "\n",
    "    # Shape 맞추기 (이 중 하나 선택)\n",
    "    # 1. ff_dim을 out1과 동일하게 맞춤\n",
    "    # ffn_output = Dense(out1.shape[-1], activation=\"relu\")(out1)\n",
    "\n",
    "    # 2. out1의 feature 크기를 ff_dim으로 확장\n",
    "    out1_resized = Dense(ff_dim)(out1)\n",
    "    out2 = Add()([out1_resized, ffn_output])\n",
    "\n",
    "    # Layer Normalization\n",
    "    out2 = LayerNormalization(epsilon=1e-6)(out2)\n",
    "    return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3670833-be9b-422d-a2e2-7f405f47866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 모델 생성\n",
    "def create_fully_transformer_model(input_shape, output_dim, num_heads=4, key_dim=32, ff_dim=128, dropout_rate=0.1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # First Transformer Block\n",
    "    x = transformer_block(inputs, num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    # Second Transformer Block (추가)\n",
    "    x = transformer_block(x, num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    # Third Transformer Block (추가)\n",
    "    x = transformer_block(x, num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    # Fourth Transformer Block (추가)\n",
    "    x = transformer_block(x, num_heads=num_heads, key_dim=key_dim, ff_dim=ff_dim, dropout_rate=dropout_rate)\n",
    "\n",
    "    # Global Pooling to Flatten Time Steps\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # Additional Dense Layer (추가)\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)  # 추가 Dropout\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(output_dim)(x)\n",
    "\n",
    "    # Compile Model\n",
    "    optimizer = Adam(learning_rate=0.0001)  # 학습률 설정\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer=optimizer, loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1. 부트스트랩 학습 함수\n",
    "def bootstrap_training(X, y, create_model_fn, n_bootstrap=5, epochs=1000, batch_size=32, validation_split=0.2):\n",
    "    models = []\n",
    "    bootstrap_results = []\n",
    "    # EarlyStopping 콜백 정의\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',       # 모니터링할 값 ('val_loss', 'val_mae' 등)\n",
    "        patience=10,              # 개선되지 않는 epoch 수\n",
    "        restore_best_weights=True # 가장 좋은 모델의 가중치를 복원\n",
    "    )\n",
    "    for i in range(n_bootstrap):\n",
    "        print(f\"Training on Bootstrap Sample {i+1}/{n_bootstrap}\")\n",
    "\n",
    "        # 부트스트랩 샘플 생성 (shape 유지)\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        X_bootstrap = X[indices]\n",
    "        y_bootstrap = y[indices]\n",
    "\n",
    "        # 모델 생성\n",
    "        model = create_model_fn()\n",
    "\n",
    "        # 모델 학습\n",
    "        history = model.fit(\n",
    "            X_bootstrap, y_bootstrap,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            verbose=1,\n",
    "            callbacks=[early_stopping]  # EarlyStopping 콜백 추가\n",
    "        )\n",
    "\n",
    "        # 학습 결과 저장\n",
    "        models.append(model)\n",
    "        val_loss = min(history.history['val_loss'])\n",
    "        bootstrap_results.append(val_loss)\n",
    "\n",
    "        print(f\"Bootstrap {i+1}: Best Validation Loss = {val_loss:.4f}\")\n",
    "\n",
    "    return models, bootstrap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb3088a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앙상블 예측 함수\n",
    "def ensemble_predict(models, X_test):\n",
    "    predictions = [model.predict(X_test) for model in models]\n",
    "    return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201224e-3c05-4349-80e1-690f934c8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ts, y_ts, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a98338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 부트스트랩 학습 실행\n",
    "n_bootstrap = 10\n",
    "models, results = bootstrap_training(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    lambda: create_fully_transformer_model(input_shape=(30, 9), output_dim=9), \n",
    "    n_bootstrap=n_bootstrap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f724022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 앙상블 예측 및 평가\n",
    "y_pred = ensemble_predict(models, X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred, multioutput='raw_values')\n",
    "mse = mean_squared_error(y_test, y_pred, multioutput='raw_values')\n",
    "\n",
    "print(\"Validation Loss from Bootstrap Samples:\", results)\n",
    "print(\"Mean Validation Loss:\", np.mean(results))\n",
    "print(\"MAE per output variable:\", mae)\n",
    "print(\"MSE per output variable:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49353434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원래 데이터 복원\n",
    "def reconstruct_from_features(X_features, lag):\n",
    "    reconstructed = []\n",
    "    for window in X_features:\n",
    "        reconstructed.extend(window[:1])  # 각 윈도우의 첫 번째 값만 복원\n",
    "    reconstructed = np.array(reconstructed)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a515df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 복원된 데이터 생성\n",
    "reconstructed_data = reconstruct_from_features(X_ts, lag)\n",
    "\n",
    "y_pred = ensemble_predict(models, X_ts)\n",
    "\n",
    "# 실제값과 예측값을 비교 (시각화)\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(range(len(y)), y, label=\"Original Data\", color=\"blue\", linestyle=\"--\", alpha=0.6)\n",
    "plt.plot(range(lag, lag + len(y_ts)), y_pred, label=\"Predicted Data\", color=\"red\", alpha=0.8)\n",
    "plt.title(\"Comparison of Actual vs Predicted Data\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 데이터 비교 출력\n",
    "print(f\"Original Data Shape: {y.shape}\")\n",
    "print(f\"Reconstructed Data Shape: {reconstructed_data.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
