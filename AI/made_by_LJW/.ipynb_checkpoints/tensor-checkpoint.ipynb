{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensor Flow를 활용한 데이터 회귀 분석</h3>\n",
    "\n",
    "텐서플로우(TensorFlow)를 사용하여 기후 데이터를 기반으로 뉴스 수를 예측하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 필요 패키지 목록 정의\n",
    "# packages = [\n",
    "#     \"numpy\",\n",
    "#     \"pandas\",\n",
    "#     \"tensorflow\",\n",
    "#     \"scikit-learn\",\n",
    "#     \"sklearn\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 필요 패키지 설치\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# def check_and_install_packages(package_list):\n",
    "#     for package in package_list:\n",
    "#         package_name = package.split(\"==\")[0]\n",
    "#         try:\n",
    "#             __import__(package_name)\n",
    "#             print(f\"✔ '{package}' is already installed.\")\n",
    "#         except ImportError:\n",
    "#             print(f\"✘ '{package}' is not installed. Installing now...\")\n",
    "#             try:\n",
    "#                 subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "#                 print(f\"✔ '{package}' installed successfully.\")\n",
    "#             except subprocess.CalledProcessError:\n",
    "#                 print(f\"✘ Failed to install '{package}'. Please install it manually.\")\n",
    "\n",
    "# check_and_install_packages(packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import LogCosh\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path : C:\\Users\\limul\\Desktop\\College\\2_grade\\2_semester\\computational_statistics\\teamProject\\2024-2-CSP\\AI\\made_by_LJW\\..\\data_preprocessing\\merged_final_data.csv\n",
      "['datetime', 'Article_Num', 'Polution', 'Enviroment_Polution', 'Biodiversity_Loss', 'Acid_Rain', 'Water_Pollution', 'Climate_Crisis', 'Accelerated_Global_Warming', 'Ozone_Layer_Depletion', 'Hazardous_Substance_Leakage', 'Carbon_Dioxide', 'Weekly_News_Count', 'News_Ratio', 'tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'precip', 'windspeed', 'sealevelpressure', 'moonphase']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로드\n",
    "# 뉴스 데이터 로드\n",
    "datasPath = os.path.join(os.getcwd(), \"..\", \"data_preprocessing\", \"merged_final_data.csv\")\n",
    "print(f\"file path : {datasPath}\")\n",
    "data = pd.read_csv(datasPath, encoding='utf-8')\n",
    "\n",
    "# Week_Num 속성 제거\n",
    "data = data.drop(columns=['Week_Num'])\n",
    "\n",
    "data.head()\n",
    "data_vars_list = list(data.columns)\n",
    "\n",
    "print(data_vars_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      datetime  Article_Num  Polution  Enviroment_Polution  Biodiversity_Loss  \\\n",
      "0  946771200.0           10        12                    0                  0   \n",
      "1  947376000.0           28        33                    0                  0   \n",
      "2  947980800.0           16        14                    0                  0   \n",
      "3  948585600.0           19        24                    0                  0   \n",
      "4  949190400.0           10        16                    1                  0   \n",
      "\n",
      "   Acid_Rain  Water_Pollution  Climate_Crisis  Accelerated_Global_Warming  \\\n",
      "0          5                0               0                           0   \n",
      "1          4                0               0                           0   \n",
      "2          1                0               0                           0   \n",
      "3          2                0               0                           0   \n",
      "4          3                0               0                           0   \n",
      "\n",
      "   Ozone_Layer_Depletion  ...  News_Ratio    tempmax    tempmin       temp  \\\n",
      "0                      0  ...    0.000336  47.600000  29.300000  41.450000   \n",
      "1                      1  ...    0.000942  35.714286  21.342857  29.128571   \n",
      "2                      1  ...    0.000538  38.871429  24.857143  32.785714   \n",
      "3                      0  ...    0.000639  35.242857  18.414286  26.142857   \n",
      "4                      0  ...    0.000336  32.000000  14.128571  23.357143   \n",
      "\n",
      "         dew   humidity    precip  windspeed  sealevelpressure  moonphase  \n",
      "0  31.500000  68.350000  0.117000  10.050000       1020.850000   0.830000  \n",
      "1  19.485714  68.742857  0.109143   9.971429       1022.157143   0.550000  \n",
      "2  23.485714  69.671429  0.037714  10.000000       1022.942857   0.220000  \n",
      "3  15.528571  65.642857  0.019571   9.742857       1027.285714   0.461429  \n",
      "4  10.900000  60.342857  0.003857  10.800000       1032.157143   0.697143  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "X_train\n",
      "[[0.60303514 0.04096386 0.17557252 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.04195804\n",
      "  0.83521984 0.03414056]\n",
      " [0.18290735 0.02891566 0.11450382 0.         0.         0.\n",
      "  0.33333333 0.         0.         0.         0.         0.03496503\n",
      "  0.12860861 0.07701556]\n",
      " [0.45367412 0.11325301 0.26717557 0.         0.         0.06451613\n",
      "  0.         0.         0.         0.         0.         0.22377622\n",
      "  0.67615222 0.12856446]\n",
      " [0.38178914 0.17108434 0.48091603 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.25874126\n",
      "  0.45452939 0.25678433]\n",
      " [0.9313099  0.38313253 0.27480916 0.         0.         0.\n",
      "  0.         0.05263158 0.         0.         0.         0.23776224\n",
      "  0.81047337 0.39796545]]\n",
      "\n",
      "y_train\n",
      "[[0.80053293 0.8246515  0.80064553 0.86128625 0.59683051 0.00174882\n",
      "  0.20983319 0.19315721 0.7311828 ]\n",
      " [0.65511991 0.68433602 0.65217391 0.72209962 0.53263497 0.00272347\n",
      "  0.48463565 0.15288003 0.30721966]\n",
      " [0.38180434 0.34152605 0.35921777 0.46421816 0.41149611 0.\n",
      "  0.20105356 0.70853183 0.38095238]\n",
      " [0.3467834  0.32190022 0.33282704 0.3970681  0.26000537 0.\n",
      "  0.26953468 0.66089216 0.31643625]\n",
      " [0.65812714 0.63587674 0.64359218 0.22745902 0.25248456 0.00739932\n",
      "  0.64793679 0.52706799 0.11674347]]\n"
     ]
    }
   ],
   "source": [
    "# 입력과 출력 컬럼 정의\n",
    "X_columns = ['datetime', 'Article_Num', 'Polution', 'Enviroment_Polution', 'Biodiversity_Loss',\n",
    "             'Acid_Rain', 'Water_Pollution', 'Climate_Crisis', 'Accelerated_Global_Warming',\n",
    "             'Ozone_Layer_Depletion', 'Hazardous_Substance_Leakage', 'Carbon_Dioxide',\n",
    "             'Weekly_News_Count', 'News_Ratio']\n",
    "y_columns = ['tempmax', 'tempmin', 'temp', 'dew', 'humidity', 'precip', \n",
    "             'windspeed', 'sealevelpressure', 'moonphase']\n",
    "\n",
    "\n",
    "# String data인 datetime을 Unix Timestamp로 변환\n",
    "data['datetime'] = pd.to_datetime(data['datetime'])  # 날짜를 datetime 객체로 변환\n",
    "data['datetime'] = data['datetime'].map(pd.Timestamp.timestamp)  # Unix Timestamp로 변환\n",
    "\n",
    "# # datetime을 제외하고 데이터 스케일링\n",
    "# datetime_tmp = data['datetime']\n",
    "# data = data.drop(columns='datetime')\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_data = scaler.fit_transform(data)\n",
    "# scaled_data = pd.DataFrame(scaled_data, columns=data.columns)\n",
    "# data = pd.concat([datetime_tmp, scaled_data], axis=1)\n",
    "print(data.head())\n",
    "\n",
    "# 데이터 분리\n",
    "X = data[X_columns]\n",
    "y = data[y_columns]\n",
    "\n",
    "# 훈련 및 테스트 데이터 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# 스케일링\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "X_train = x_scaler.fit_transform(X_train)\n",
    "y_train = y_scaler.fit_transform(y_train)\n",
    "\n",
    "X_test = x_scaler.transform(X_test)\n",
    "y_test = y_scaler.transform(y_test)\n",
    "\n",
    "\n",
    "print(\"X_train\")\n",
    "print(X_train[:5])\n",
    "print()\n",
    "print(\"y_train\")\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 과정 정의\n",
    "epoch = 1000\n",
    "lr = 0.0001\n",
    "\n",
    "# 사용자 정의 옵티마이저와 손실 함수\n",
    "# 손실 함수는 이상치에 대해 Log-Cosh Loss함수 사용\n",
    "# MAE와 유사하지만, 큰 오차에 대해 더 부드럽게 처리 가능\n",
    "optimizer = Adam(learning_rate=lr)  # Adam 옵티마이저, 학습률 0.001\n",
    "loss_function = LogCosh()\n",
    "\n",
    "# 모델 생성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    tf.keras.layers.Dense(len(y_columns))  # 출력 노드 수는 y_columns의 수와 동일\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "15/15 [==============================] - 0s 9ms/step - loss: 0.1351 - mae: 0.4628 - val_loss: 0.1314 - val_mae: 0.4534\n",
      "Epoch 2/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1291 - mae: 0.4497 - val_loss: 0.1251 - val_mae: 0.4396\n",
      "Epoch 3/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1225 - mae: 0.4360 - val_loss: 0.1185 - val_mae: 0.4248\n",
      "Epoch 4/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1164 - mae: 0.4219 - val_loss: 0.1114 - val_mae: 0.4086\n",
      "Epoch 5/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1096 - mae: 0.4063 - val_loss: 0.1040 - val_mae: 0.3913\n",
      "Epoch 6/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1034 - mae: 0.3916 - val_loss: 0.0966 - val_mae: 0.3736\n",
      "Epoch 7/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0974 - mae: 0.3764 - val_loss: 0.0895 - val_mae: 0.3559\n",
      "Epoch 8/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0913 - mae: 0.3609 - val_loss: 0.0827 - val_mae: 0.3389\n",
      "Epoch 9/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0856 - mae: 0.3464 - val_loss: 0.0764 - val_mae: 0.3229\n",
      "Epoch 10/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0812 - mae: 0.3355 - val_loss: 0.0708 - val_mae: 0.3086\n",
      "Epoch 11/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0758 - mae: 0.3213 - val_loss: 0.0657 - val_mae: 0.2957\n",
      "Epoch 12/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0723 - mae: 0.3128 - val_loss: 0.0611 - val_mae: 0.2842\n",
      "Epoch 13/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0696 - mae: 0.3078 - val_loss: 0.0574 - val_mae: 0.2745\n",
      "Epoch 14/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0663 - mae: 0.2972 - val_loss: 0.0541 - val_mae: 0.2658\n",
      "Epoch 15/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0639 - mae: 0.2939 - val_loss: 0.0514 - val_mae: 0.2588\n",
      "Epoch 16/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0612 - mae: 0.2853 - val_loss: 0.0487 - val_mae: 0.2509\n",
      "Epoch 17/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0591 - mae: 0.2803 - val_loss: 0.0462 - val_mae: 0.2432\n",
      "Epoch 18/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0577 - mae: 0.2752 - val_loss: 0.0441 - val_mae: 0.2366\n",
      "Epoch 19/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0553 - mae: 0.2704 - val_loss: 0.0423 - val_mae: 0.2314\n",
      "Epoch 20/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0544 - mae: 0.2664 - val_loss: 0.0408 - val_mae: 0.2269\n",
      "Epoch 21/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0521 - mae: 0.2620 - val_loss: 0.0391 - val_mae: 0.2222\n",
      "Epoch 22/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0503 - mae: 0.2563 - val_loss: 0.0374 - val_mae: 0.2180\n",
      "Epoch 23/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0492 - mae: 0.2542 - val_loss: 0.0359 - val_mae: 0.2130\n",
      "Epoch 24/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0465 - mae: 0.2455 - val_loss: 0.0340 - val_mae: 0.2077\n",
      "Epoch 25/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0452 - mae: 0.2421 - val_loss: 0.0323 - val_mae: 0.2029\n",
      "Epoch 26/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0448 - mae: 0.2422 - val_loss: 0.0313 - val_mae: 0.1997\n",
      "Epoch 27/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0438 - mae: 0.2410 - val_loss: 0.0300 - val_mae: 0.1962\n",
      "Epoch 28/1000\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0425 - mae: 0.2355 - val_loss: 0.0291 - val_mae: 0.1936\n",
      "Epoch 29/1000\n",
      " 1/15 [=>............................] - ETA: 0s - loss: 0.0389 - mae: 0.2251"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(X_train, y_train, epochs=epoch, batch_size=64, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# 예측 수행\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 출력 데이터(y_columns)에 대한 스케일러를 사용해 역변환\n",
    "y_test_inverse = y_scaler.inverse_transform(y_test)  # 실제 값 복원\n",
    "y_pred_inverse = y_scaler.inverse_transform(y_pred)  # 예측 값 복원\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(\"\\nSample Predictions:\")\n",
    "for i in range(5):  # 처음 5개의 샘플에 대해 출력\n",
    "    true_values = y_test_inverse[i]  # Numpy 배열에서 직접 값 가져오기\n",
    "    predicted_values = y_pred_inverse[i]  # 예측 값\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    for attr, true_val, pred_val in zip(y_columns, true_values, predicted_values):\n",
    "        print(f\"  {attr:<18} | True: {true_val} | Predicted: {pred_val}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 데이터 분리\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # 스케일러 생성 및 y_train 기준으로 fit\n",
    "# y_scaler = MinMaxScaler()\n",
    "# y_train_scaled = y_scaler.fit_transform(y_train)  # y_train을 기준으로 스케일링\n",
    "\n",
    "# # y_test도 동일한 스케일러로 transform\n",
    "# y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "# # y_test 복원 (역변환)\n",
    "# y_test_restored = y_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# # 결과 출력\n",
    "# print(\"Original y_test:\\n\", y_test.head())\n",
    "# print(\"\\nScaled y_test:\\n\", y_test_scaled[:5])\n",
    "# print(\"\\nRestored y_test:\\n\", y_test_restored[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
