{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fft import fft\n",
    "from torch.optim import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 경로 설정\n",
    "pollutants_path = '../../NewData/Weekly_Power_Usage.csv'\n",
    "weather_path = '../data_preprocessing/normalized_analyze_abnormal.csv'\n",
    "\n",
    "# 데이터 로드\n",
    "pollutants_data = pd.read_csv(pollutants_path)\n",
    "weather_data = pd.read_csv(weather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 정의\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # register_buffer를 사용하여 모델 디바이스에 맞춤\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.pe가 입력 텐서 x와 동일한 디바이스에 있도록 설정\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "    \n",
    "# WeatherTransformer 정의 수정\n",
    "class WeatherTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, d_model, nhead, num_layers, sequence_length, dropout=0.1):\n",
    "        super(WeatherTransformer, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_dim, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len=sequence_length)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, nhead=nhead, num_encoder_layers=num_layers, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(d_model, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_embedding(x)\n",
    "        x = self.positional_encoding(x)  # Positional Encoding 적용\n",
    "        x = x.permute(1, 0, 2)  # Transformer 입력 차원 변환 (seq_len, batch_size, d_model)\n",
    "        x = self.transformer(x, x)\n",
    "        x = x.permute(1, 0, 2)  # 다시 원래 차원으로 변환 (batch_size, seq_len, d_model)\n",
    "        x = self.fc(x.mean(dim=1))  # 출력 계산\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nadam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        super(Nadam, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Nadam does not support sparse gradients')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                state['step'] += 1\n",
    "                step = state['step']\n",
    "\n",
    "                # Update biased first and second moment estimates\n",
    "                # exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                # exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "\n",
    "                # Bias correction\n",
    "                bias_correction1 = 1 - beta1 ** step\n",
    "                bias_correction2 = 1 - beta2 ** step\n",
    "                step_size = group['lr'] * (bias_correction2 ** 0.5) / bias_correction1\n",
    "\n",
    "                # Nesterov momentum\n",
    "                nesterov_momentum = beta1 * exp_avg + (1 - beta1) * grad\n",
    "                # p.data.add_(-step_size, nesterov_momentum)\n",
    "                p.data.add_(nesterov_momentum, alpha=-step_size)\n",
    "\n",
    "\n",
    "                # Apply weight decay if needed\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p.data.add_(-group['weight_decay'], p.data)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['TotalEmissions', 'NetEmissions', 'EnergyTotal', 'EnergyCombustionFuel', 'EnergyFugitive', 'IndustrialProcesses', 'Agriculture', 'LandUseChangeForestry', 'Waste'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     18\u001b[0m input_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotalEmissions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNetEmissions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergyTotal\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergyCombustionFuel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnergyFugitive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndustrialProcesses\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgriculture\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLandUseChangeForestry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaste\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     19\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_of_year\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweek_of_year\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_sin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_cos\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     20\u001b[0m target_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhumidity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwindspeed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 22\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# 입력 데이터\u001b[39;00m\n\u001b[0;32m     23\u001b[0m Y \u001b[38;5;241m=\u001b[39m merged_data[target_features]\u001b[38;5;241m.\u001b[39mvalues  \u001b[38;5;66;03m# 타겟 데이터\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Fourier 변환\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\tf-gpu\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['TotalEmissions', 'NetEmissions', 'EnergyTotal', 'EnergyCombustionFuel', 'EnergyFugitive', 'IndustrialProcesses', 'Agriculture', 'LandUseChangeForestry', 'Waste'] not in index\""
     ]
    }
   ],
   "source": [
    "# 데이터 병합\n",
    "pollutants_data['datetime'] = pd.to_datetime(pollutants_data['datetime'])\n",
    "weather_data['datetime'] = pd.to_datetime(weather_data['datetime'])\n",
    "merged_data = pd.merge(weather_data, pollutants_data, on='datetime', how='inner')\n",
    "\n",
    "# 시간 정보 추가\n",
    "merged_data['day_of_year'] = merged_data['datetime'].dt.dayofyear\n",
    "merged_data['week_of_year'] = merged_data['datetime'].dt.isocalendar().week\n",
    "merged_data['year'] = merged_data['datetime'].dt.year\n",
    "\n",
    "# 시간 정보 추가\n",
    "merged_data['day_of_year'] = merged_data['datetime'].dt.dayofyear\n",
    "merged_data['day_sin'] = np.sin(2 * np.pi * merged_data['day_of_year'] / 365.0)\n",
    "merged_data['day_cos'] = np.cos(2 * np.pi * merged_data['day_of_year'] / 365.0)\n",
    "\n",
    "\n",
    "# 입력(X)와 출력(Y) 설정\n",
    "input_features = [\"Residential\", \"General\", \"Educational\", \"Industrial\", \"Agricultural\", \"Streetlights\", \"Other\", \"Total Electricity Usage\", \n",
    "                  'day_of_year', 'week_of_year', 'year', 'day_sin', 'day_cos']\n",
    "target_features = ['temp', 'humidity', 'precip', 'windspeed']\n",
    "\n",
    "X = merged_data[input_features].values  # 입력 데이터\n",
    "Y = merged_data[target_features].values  # 타겟 데이터\n",
    "\n",
    "# Fourier 변환\n",
    "fourier_transform = fft(Y[:, 0])  # 첫 번째 타겟 변수 temp 사용\n",
    "frequencies = np.abs(fourier_transform)  # 주파수 성분의 크기 계산\n",
    "\n",
    "# 주요 주파수 성분 추출\n",
    "n_top_frequencies = 5  # 사용할 주파수 성분의 개수\n",
    "top_frequencies = frequencies[:len(frequencies) // 2]  # 절반까지만 사용\n",
    "\n",
    "# Fourier 변환 성분을 새로운 특징으로 추가\n",
    "for i in range(n_top_frequencies):\n",
    "    merged_data[f'fft_freq_{i+1}'] = top_frequencies[i]\n",
    "\n",
    "# Fourier 변환으로 생성된 특징 추가\n",
    "input_features += [f'fft_freq_{i+1}' for i in range(n_top_frequencies)]\n",
    "\n",
    "# 입력 데이터 X 재생성\n",
    "X = merged_data[input_features].values\n",
    "\n",
    "print(\"입력 데이터 X의 형태:\", X.shape)\n",
    "print(\"출력 데이터 Y의 형태:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 슬라이딩 윈도우로 시계열 데이터 생성\n",
    "sequence_length = 7\n",
    "def create_sequences(data, target, sequence_length):\n",
    "    X_seq, Y_seq = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        X_seq.append(data[i:i+sequence_length])  # 슬라이딩 윈도우\n",
    "        if target is not None:\n",
    "            Y_seq.append(target[i+sequence_length])  # 다음 시점의 모든 출력값\n",
    "    return np.array(X_seq), np.array(Y_seq)  # 3D (samples, sequence_length, features), 2D (samples, output_dim)\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# MinMaxScaler 적용 (2D 형태로 처리)\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_Y = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "Y_train_scaled = scaler_Y.fit_transform(Y_train)  # (samples, output_dim)\n",
    "Y_test_scaled = scaler_Y.transform(Y_test)\n",
    "\n",
    "# 슬라이딩 윈도우로 시계열 데이터 생성\n",
    "X_train_seq, Y_train_seq = create_sequences(X_train_scaled, Y_train_scaled, sequence_length)\n",
    "X_test_seq, Y_test_seq = create_sequences(X_test_scaled, Y_test_scaled, sequence_length)\n",
    "\n",
    "# Tensor 변환\n",
    "X_train_tensor = torch.tensor(X_train_seq, dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_seq, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_seq, dtype=torch.float32)\n",
    "Y_test_tensor = torch.tensor(Y_test_seq, dtype=torch.float32)\n",
    "\n",
    "# DataLoader 생성\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, Y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, Y_test_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 모델 출력 확인\n",
    "print(f\"Model output size: {len(target_features)}\")  # 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 설정\n",
    "input_dim = len(input_features)\n",
    "output_dim = len(target_features)\n",
    "d_model = 1280\n",
    "nhead = 64\n",
    "num_layers = 20\n",
    "dropout = 0.05\n",
    "learning_rate = 0.0005\n",
    "epochs = 200\n",
    "\n",
    "# Early Stopping 설정\n",
    "early_stopping_patience = 5\n",
    "best_train_loss = float('inf')\n",
    "best_test_loss = float('inf')\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 device 설정\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda\")  # GPU 사용\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")  # CPU 사용\n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = X_train_seq.shape[-1]  # 입력 데이터의 특징 차원\n",
    "model = WeatherTransformer(\n",
    "    input_dim=input_dim,\n",
    "    output_dim=len(target_features),\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_layers=num_layers,\n",
    "    sequence_length=sequence_length,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "# criterion = nn.MSELoss()\n",
    "# beta값은 실험을 통해 줄이거나 늘려야함\n",
    "criterion = nn.SmoothL1Loss(beta=0.5)  # beta는 Huber Loss의 delta 값에 해당\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = Nadam(model.parameters(), lr=learning_rate)   # optimizer 변경\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # 데이터를 GPU 또는 CPU로 이동\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # 검증\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, Y_batch in test_loader:\n",
    "            # 데이터를 GPU 또는 CPU로 이동\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "            \n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    scheduler.step(avg_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    if avg_train_loss < best_train_loss or avg_test_loss < best_test_loss:\n",
    "        best_train_loss, best_test_loss = avg_train_loss, avg_test_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model_v2.pth\")\n",
    "    else:\n",
    "        if epoch > 100:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement for {patience_counter} epochs.\")\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 모델 로드 및 평가 모드 설정\n",
    "model.load_state_dict(torch.load(\"best_model_v2.pth\"))\n",
    "model.eval()  # 모델을 평가 모드로 전환\n",
    "\n",
    "# 전체 데이터 슬라이딩 윈도우로 시계열 생성\n",
    "X_seq, Y_seq = create_sequences(X, Y, sequence_length)\n",
    "\n",
    "# 데이터 스케일링\n",
    "X_seq_scaled = scaler_X.transform(X_seq.reshape(-1, X_seq.shape[-1])).reshape(X_seq.shape)\n",
    "Y_seq_scaled = scaler_Y.transform(Y_seq)\n",
    "\n",
    "# Tensor 변환 후 GPU 또는 CPU로 이동\n",
    "X_tensor = torch.tensor(X_seq_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "# 예측 수행\n",
    "with torch.no_grad():\n",
    "    predictions_scaled = model(X_tensor).cpu().numpy()  # 모델의 출력은 CPU로 이동 후 NumPy 변환\n",
    "\n",
    "# 스케일 역변환\n",
    "predictions = scaler_Y.inverse_transform(predictions_scaled)\n",
    "actual_values = scaler_Y.inverse_transform(Y_seq_scaled)\n",
    "\n",
    "# 평가\n",
    "rmse = mean_squared_error(actual_values, predictions, squared=False)\n",
    "mae = mean_absolute_error(actual_values, predictions)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 (예: 첫 번째 출력 변수)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(actual_values[:, 0], label=\"Actual (temp)\", color=\"blue\")\n",
    "plt.plot(predictions[:, 0], label=\"Predicted (temp)\", color=\"orange\")\n",
    "plt.legend()\n",
    "plt.title(\"Actual vs Predicted (First Target Feature)\")\n",
    "plt.xlabel(\"Time Steps\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
